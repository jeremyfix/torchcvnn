{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Complex Valued Neural Networks","text":"<p>This is a library that uses pytorch as a back-end for complex valued neural networks.</p> <p>It was initially developed by Victor Dh\u00e9din and J\u00e9r\u00e9mie Levi during their third year project at CentraleSup\u00e9lec. </p>"},{"location":"#installation","title":"Installation","text":"<p>To install the library, it is simple as :</p> <pre><code>pip install git+ssh://git@github.com/jeremyfix/torchcvnn.git\n</code></pre>"},{"location":"#other-projects","title":"Other projects","text":"<p>You might also be interested in some other projects: </p> <p>Tensorflow based : </p> <ul> <li>cvnn developed by colleagues from CentraleSup\u00e9lec</li> </ul> <p>Pytorch based : </p> <ul> <li>cplxmodule</li> <li>complexPyTorch</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#using-the-source-code","title":"Using the source code","text":"<pre><code>python3 -m pip install git+https://github.com/jeremyfix/torchcvnn.git\n</code></pre>"},{"location":"installation/#using-pip","title":"Using pip","text":"<pre><code>python3 -m pip install torchcvnn\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>torchcvnn<ul> <li>datasets<ul> <li>alos2<ul> <li>dataset</li> <li>leader_file</li> <li>parse_utils</li> <li>sar_image</li> <li>trailer_file</li> <li>vol_file</li> </ul> </li> <li>bretigny<ul> <li>dataset</li> </ul> </li> <li>polsf</li> <li>slc<ul> <li>ann_file</li> <li>dataset</li> <li>slc_file</li> </ul> </li> </ul> </li> <li>nn<ul> <li>modules<ul> <li>activation</li> <li>batchnorm</li> <li>conv</li> <li>dropout</li> <li>initialization</li> <li>loss</li> <li>normalization</li> <li>pooling</li> <li>upsampling</li> </ul> </li> </ul> </li> </ul> </li> <li>Tutorials<ul> <li>MNIST</li> <li>ALOS2</li> <li>SLC</li> </ul> </li> </ul>"},{"location":"reference/examples/alos2/","title":"ALOS2","text":"<p>Example script to read ALOS2 data</p> Requires additional dependencies <p>python3 -m pip install matplotlib scikit-image</p>"},{"location":"reference/examples/alos2/#read_alos2.plot_patches","title":"plot_patches","text":"<pre><code>plot_patches(rootdir)\n</code></pre> <p>Loads a ALOS-2 dataset and display some extracted patches</p> Source code in <code>examples/read_alos2.py</code> <pre><code>def plot_patches(rootdir):\n    \"\"\"\n    Loads a ALOS-2 dataset and display some extracted patches\n    \"\"\"\n    # Get and find a VOL file in the provided path\n    vol_filepath = get_volfile(rootdir)\n\n    # Limit to a subpart of the ALOS-2 data\n    # This corresponds to the annotated region of the PolSF dataset\n    crop_coordinates = ((2832, 736), (7888, 3520))\n    dataset = alos2.ALOSDataset(\n        vol_filepath,\n        patch_size=(512, 512),\n        patch_stride=(128, 128),\n        crop_coordinates=crop_coordinates,\n    )\n\n    # Plot consecutive samples\n    fig, axes = plt.subplots(1, 4, figsize=(10, 4))\n    for i, ax in enumerate(axes):\n        X = dataset[i]\n        X = X[:, ::-1, :]\n        xi = X[0]\n        norm_xi = normalize(xi)\n        ax.imshow(norm_xi, cmap=\"gray\")\n        ax.set_title(f\"Sample {i}\")\n        ax.axis(\"off\")\n    plt.tight_layout()\n\n    # Plot the four polarizations of the same patch\n    X = dataset[0]\n    X = X[:, ::-1, :]\n    fig, axes = plt.subplots(1, 4, figsize=(10, 4))\n    for xi, ax, ax_title in zip(X, axes, [\"HH\", \"HV\", \"VH\", \"VV\"]):\n        norm_xi = normalize(xi)\n        ax.imshow(norm_xi, cmap=\"gray\")\n        ax.axis(\"off\")\n        ax.set_title(ax_title)\n    plt.tight_layout()\n\n    plt.show()\n</code></pre>"},{"location":"reference/examples/alos2/#read_alos2.plot_summaries","title":"plot_summaries","text":"<pre><code>plot_summaries(rootdir)\n</code></pre> <p>Loads a ALOS-2 dataset and prints some informations extracted from the Volume, Leader, Trailer and Image files</p> Source code in <code>examples/read_alos2.py</code> <pre><code>def plot_summaries(rootdir):\n    \"\"\"\n    Loads a ALOS-2 dataset and prints some informations extracted from the\n    Volume, Leader, Trailer and Image files\n    \"\"\"\n    # Get and find a VOL file in the provided path\n    vol_filepath = get_volfile(rootdir)\n\n    # Parse the data\n    dataset = alos2.ALOSDataset(vol_filepath)\n\n    # And print some decoded infos\n    dataset.describe()\n</code></pre>"},{"location":"reference/examples/mnist/","title":"MNIST","text":""},{"location":"reference/examples/mnist/#mnist--example-using-complex-valued-neural-networks-to-classify-mnist-from-the-fourier-transform-of-the-digits","title":"Example using complex valued neural networks to classify MNIST from the Fourier Transform of the digits.","text":"Requires dependencies <p>python3 -m pip install torchvision tqdm</p>"},{"location":"reference/examples/mnist/#mnist.conv_block","title":"conv_block","text":"<pre><code>conv_block(\n    in_c: int, out_c: int, cdtype: dtype\n) -&gt; List[Module]\n</code></pre> <p>Builds a basic building block of <code>Conv2d</code>-<code>Cardioid</code>-<code>Conv2d</code>-<code>Cardioid</code>-<code>AvgPool2d</code></p> <p>Parameters:</p> <ul> <li> <code>in_c</code>         \u2013          <p>the number of input channels</p> </li> <li> <code>out_c</code>         \u2013          <p>the number of output channels</p> </li> <li> <code>cdtype</code>         \u2013          <p>the dtype of complex values (expected to be torch.complex64 or torch.complex32)</p> </li> </ul> Source code in <code>examples/mnist.py</code> <pre><code>def conv_block(in_c: int, out_c: int, cdtype: torch.dtype) -&gt; List[nn.Module]:\n    \"\"\"\n    Builds a basic building block of\n    `Conv2d`-`Cardioid`-`Conv2d`-`Cardioid`-`AvgPool2d`\n\n    Arguments:\n        in_c : the number of input channels\n        out_c : the number of output channels\n        cdtype : the dtype of complex values (expected to be torch.complex64 or torch.complex32)\n    \"\"\"\n    return [\n        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1, dtype=cdtype),\n        c_nn.BatchNorm2d(out_c),\n        c_nn.Cardioid(),\n        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1, dtype=cdtype),\n        c_nn.BatchNorm2d(out_c),\n        c_nn.Cardioid(),\n        c_nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n    ]\n</code></pre>"},{"location":"reference/examples/mnist/#mnist.train","title":"train","text":"<pre><code>train()\n</code></pre> <p>Train function</p> Sample output <pre><code>(venv) me@host:~$ python mnist.py\nLogging to ./logs/CMNIST_0\n&gt;&gt; Training\n100%|\u2588\u2588\u2588\u2588\u2588\u2588| 844/844 [00:17&lt;00:00, 48.61it/s]\n&gt;&gt; Testing\n[Step 0] Train : CE  0.20 Acc  0.94 | Valid : CE  0.08 Acc  0.97 | Test : CE 0.06 Acc  0.98[&gt;&gt; BETTER &lt;&lt;]\n\n&gt;&gt; Training\n100%|\u2588\u2588\u2588\u2588\u2588\u2588| 844/844 [00:16&lt;00:00, 51.69it/s]\n&gt;&gt; Testing\n[Step 1] Train : CE  0.06 Acc  0.98 | Valid : CE  0.06 Acc  0.98 | Test : CE 0.05 Acc  0.98[&gt;&gt; BETTER &lt;&lt;]\n\n&gt;&gt; Training\n100%|\u2588\u2588\u2588\u2588\u2588\u2588| 844/844 [00:15&lt;00:00, 53.47it/s]\n&gt;&gt; Testing\n[Step 2] Train : CE  0.04 Acc  0.99 | Valid : CE  0.04 Acc  0.99 | Test : CE 0.04 Acc  0.99[&gt;&gt; BETTER &lt;&lt;]\n\n[...]\n</code></pre> Source code in <code>examples/mnist.py</code> <pre><code>def train():\n    \"\"\"\n    Train function\n\n    Sample output :\n        ```.bash\n        (venv) me@host:~$ python mnist.py\n        Logging to ./logs/CMNIST_0\n        &gt;&gt; Training\n        100%|\u2588\u2588\u2588\u2588\u2588\u2588| 844/844 [00:17&lt;00:00, 48.61it/s]\n        &gt;&gt; Testing\n        [Step 0] Train : CE  0.20 Acc  0.94 | Valid : CE  0.08 Acc  0.97 | Test : CE 0.06 Acc  0.98[&gt;&gt; BETTER &lt;&lt;]\n\n        &gt;&gt; Training\n        100%|\u2588\u2588\u2588\u2588\u2588\u2588| 844/844 [00:16&lt;00:00, 51.69it/s]\n        &gt;&gt; Testing\n        [Step 1] Train : CE  0.06 Acc  0.98 | Valid : CE  0.06 Acc  0.98 | Test : CE 0.05 Acc  0.98[&gt;&gt; BETTER &lt;&lt;]\n\n        &gt;&gt; Training\n        100%|\u2588\u2588\u2588\u2588\u2588\u2588| 844/844 [00:15&lt;00:00, 53.47it/s]\n        &gt;&gt; Testing\n        [Step 2] Train : CE  0.04 Acc  0.99 | Valid : CE  0.04 Acc  0.99 | Test : CE 0.04 Acc  0.99[&gt;&gt; BETTER &lt;&lt;]\n\n        [...]\n        ```\n\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    valid_ratio = 0.1\n    batch_size = 64\n    epochs = 10\n    cdtype = torch.complex64\n\n    # Dataloading\n    train_valid_dataset = torchvision.datasets.MNIST(\n        root=\"./data\",\n        train=True,\n        download=True,\n        transform=v2_transforms.Compose([v2_transforms.PILToTensor(), torch.fft.fft]),\n    )\n    test_dataset = torchvision.datasets.MNIST(\n        root=\"./data\",\n        train=False,\n        download=True,\n        transform=v2_transforms.Compose([v2_transforms.PILToTensor(), torch.fft.fft]),\n    )\n\n    all_indices = list(range(len(train_valid_dataset)))\n    random.shuffle(all_indices)\n    split_idx = int(valid_ratio * len(train_valid_dataset))\n    valid_indices, train_indices = all_indices[:split_idx], all_indices[split_idx:]\n\n    # Train dataloader\n    train_dataset = torch.utils.data.Subset(train_valid_dataset, train_indices)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True\n    )\n\n    # Valid dataloader\n    valid_dataset = torch.utils.data.Subset(train_valid_dataset, valid_indices)\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size=batch_size, shuffle=False\n    )\n\n    # Test dataloader\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False\n    )\n\n    # Model\n    conv_model = nn.Sequential(\n        *conv_block(1, 16, cdtype),\n        *conv_block(16, 16, cdtype),\n        *conv_block(16, 32, cdtype),\n        *conv_block(32, 32, cdtype),\n        nn.Flatten(),\n    )\n\n    with torch.no_grad():\n        conv_model.eval()\n        dummy_input = torch.zeros((64, 1, 28, 28), dtype=cdtype, requires_grad=False)\n        out_conv = conv_model(dummy_input).view(64, -1)\n    lin_model = nn.Sequential(\n        nn.Linear(out_conv.shape[-1], 124, dtype=cdtype),\n        c_nn.Cardioid(),\n        nn.Linear(124, 10, dtype=cdtype),\n        c_nn.Mod(),\n    )\n    model = nn.Sequential(conv_model, lin_model)\n    model.to(device)\n\n    # Loss, optimizer, callbacks\n    f_loss = nn.CrossEntropyLoss()\n    optim = torch.optim.Adam(model.parameters(), lr=3e-4)\n    logpath = utils.generate_unique_logpath(\"./logs\", \"CMNIST\")\n    print(f\"Logging to {logpath}\")\n    checkpoint = utils.ModelCheckpoint(model, logpath, 4, min_is_best=True)\n\n    # Training loop\n    for e in range(epochs):\n        print(\"&gt;&gt; Training\")\n        train_loss, train_acc = utils.train_epoch(\n            model, train_loader, f_loss, optim, device\n        )\n\n        print(\"&gt;&gt; Testing\")\n        valid_loss, valid_acc = utils.test_epoch(model, valid_loader, f_loss, device)\n        test_loss, test_acc = utils.test_epoch(model, test_loader, f_loss, device)\n        updated = checkpoint.update(valid_loss)\n        better_str = \"[&gt;&gt; BETTER &lt;&lt;]\" if updated else \"\"\n\n        print(\n            f\"[Step {e}] Train : CE {train_loss:5.2f} Acc {train_acc:5.2f} | Valid : CE {valid_loss:5.2f} Acc {valid_acc:5.2f} | Test : CE {test_loss:5.2f} Acc {test_acc:5.2f}\"\n            + better_str\n        )\n</code></pre>"},{"location":"reference/examples/slc/","title":"SLC","text":"<p>Example script to read ALOS2 data</p> Requires additional dependencies <p>python3 -m pip install matplotlib scikit-image</p>"},{"location":"reference/torchcvnn/","title":"Index","text":""},{"location":"reference/torchcvnn/#torchcvnn.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = '0.5.0'\n</code></pre>"},{"location":"reference/torchcvnn/#torchcvnn.__version__--torchcvnn","title":"Torchcvnn","text":"<p>This provides several modules (layers, activation functions) suitable for optimizing complex valued neural networks with the pytorch framework.</p>"},{"location":"reference/torchcvnn/datasets/","title":"Index","text":""},{"location":"reference/torchcvnn/datasets/polsf/","title":"polsf","text":""},{"location":"reference/torchcvnn/datasets/polsf/#torchcvnn.datasets.polsf.PolSFDataset","title":"PolSFDataset","text":"<pre><code>PolSFDataset(\n    root: str,\n    transform=None,\n    patch_size: tuple = (128, 128),\n    patch_stride: tuple = None,\n)\n</code></pre> <p>             Bases: <code>Dataset</code></p> <p>The Polarimetric SAR dataset with the labels provided by https://ietr-lab.univ-rennes1.fr/polsarpro-bio/san-francisco/</p> <p>We expect the data to be already downloaded and available on your drive.</p> <p>Parameters:</p> <ul> <li> <code>root</code>             (<code>str</code>)         \u2013          <p>the top root dir where the data are expected</p> </li> <li> <code>transform</code>         \u2013          <p>the transform applied the cropped image</p> </li> <li> <code>patch_size</code>             (<code>tuple</code>, default:                 <code>(128, 128)</code> )         \u2013          <p>the dimensions of the patches to consider (rows, cols)</p> </li> <li> <code>patch_stride</code>             (<code>tuple</code>, default:                 <code>None</code> )         \u2013          <p>the shift between two consecutive patches, default:patch_size</p> </li> </ul> Note <p>An example usage :</p> <pre><code>import torchcvnn\nfrom torchcvnn.datasets import PolSFDataset\n\ndef transform_patches(patches):\n    # We keep all the patches and get the spectrum\n    # from it\n    # If you wish, you could filter out some polarizations\n    # PolSF provides the four HH, HV, VH, VV\n    patches = [np.abs(patchi) for _, patchi in patches.items()]\n    return np.stack(patches)\n\ndataset = PolSFDataset(\n    rootdir, patch_size=((512, 512)), transform=transform_patches\nX, y = dataset[0]\n</code></pre> <p>Displayed below are example patches with pache sizes \\(512 \\times 512\\) with the labels overlayed</p> <p></p> Source code in <code>src/torchcvnn/datasets/polsf.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    transform=None,\n    patch_size: tuple = (128, 128),\n    patch_stride: tuple = None,\n):\n    self.root = root\n\n    # alos2_url = \"https://ietr-lab.univ-rennes1.fr/polsarpro-bio/san-francisco/dataset/SAN_FRANCISCO_ALOS2.zip\"\n    # labels_url = \"https://raw.githubusercontent.com/liuxuvip/PolSF/master/SF-ALOS2/SF-ALOS2-label2d.png\"\n\n    crop_coordinates = ((2832, 736), (7888, 3520))\n    root = pathlib.Path(root) / \"VOL-ALOS2044980750-150324-HBQR1.1__A\"\n    self.alos_dataset = ALOSDataset(\n        root, transform, crop_coordinates, patch_size, patch_stride\n    )\n    if isinstance(root, str):\n        root = pathlib.Path(root)\n    self.labels = np.array(Image.open(root.parent / \"SF-ALOS2-label2d.png\"))[\n        ::-1, :\n    ].copy()  # copy necessary as otherwise torch.from_numpy does not support\n</code></pre>"},{"location":"reference/torchcvnn/datasets/polsf/#torchcvnn.datasets.polsf.PolSFDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx) -&gt; Tuple[Any, Any]\n</code></pre> <p>Returns the indexes patch.</p> <p>Parameters:</p> <ul> <li> <code>idx</code>             (<code>int</code>)         \u2013          <p>Index</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code> (            <code>Tuple[Any, Any]</code> )        \u2013          <p>(patch, labels) where patch contains the 4 complex valued polarization HH, HV, VH, VV and labels contains the aligned semantic labels</p> </li> </ul> Source code in <code>src/torchcvnn/datasets/polsf.py</code> <pre><code>def __getitem__(self, idx) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Returns the indexes patch.\n\n    Arguments:\n        idx (int): Index\n\n    Returns:\n        tuple: (patch, labels) where patch contains the 4 complex valued polarization HH, HV, VH, VV and labels contains the aligned semantic labels\n    \"\"\"\n    alos_patch = self.alos_dataset[idx]\n\n    row_stride, col_stride = self.alos_dataset.patch_stride\n    start_row = (idx // self.alos_dataset.nsamples_per_cols) * row_stride\n\n    start_col = (idx % self.alos_dataset.nsamples_per_cols) * col_stride\n    num_rows, num_cols = self.alos_dataset.patch_size\n    labels = self.labels[\n        start_row : (start_row + num_rows), start_col : (start_col + num_cols)\n    ]\n\n    return alos_patch, labels\n</code></pre>"},{"location":"reference/torchcvnn/datasets/polsf/#torchcvnn.datasets.polsf.PolSFDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the total number of patches in the while image.</p> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013          <p>the total number of patches in the dataset</p> </li> </ul> Source code in <code>src/torchcvnn/datasets/polsf.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the total number of patches in the while image.\n\n    Returns:\n        the total number of patches in the dataset\n    \"\"\"\n    return len(self.alos_dataset)\n</code></pre>"},{"location":"reference/torchcvnn/datasets/alos2/","title":"Index","text":"<p>Scripts to read and parse ALOS-2 data files</p> <p>The format is described in https://www.eorc.jaxa.jp/ALOS/en/alos-2/pdf/product_format_description/PALSAR-2_xx_Format_CEOS_E_g.pdf</p> <p>See also the handbook : https://www.eorc.jaxa.jp/ALOS/en/doc/alos_userhb_en.pdf for example p99 for the image line formats</p> <p>We have for High-sensitive/Fine Mode Full (Quad.) Polarimetry :     IMG-{HH, HV, VH, VV}xxxxx__A     LED-xxxx.1__A : SAR Leader file     TRL-xxxx.1__A : SAR Trailer file     VOL-xxxx.1__a : Volume directory file</p> <pre><code>With ALOS2044980750-150324-HBQR1 as Sene ID - Product ID\nSceneID : ALOS2044980750-150324\n    which stands for ALOS2 satelitte\n                     04498 : orbit accumulation number\n                     0750  : scene frame number\n                     150324 : 2015/03/24\n\nProductID : HBQR1.1__A\n    HBQ : High-sensitive mode Full (Quad.) polarimetry\n      R : Right looking\n      1.1 : Processing level\n      _ : processing option not specified\n      _ : map projection not specified\n      A : Ascending orbit direction\n</code></pre> <p>If you have different settings for the polarization, the structure of the files is described in Figure 3.1-5 p29</p>"},{"location":"reference/torchcvnn/datasets/alos2/dataset/","title":"dataset","text":""},{"location":"reference/torchcvnn/datasets/alos2/dataset/#torchcvnn.datasets.alos2.dataset.ALOSDataset","title":"ALOSDataset","text":"<pre><code>ALOSDataset(\n    volpath: str = None,\n    transform=None,\n    crop_coordinates: tuple = None,\n    patch_size: tuple = (128, 128),\n    patch_stride: tuple = None,\n)\n</code></pre> <p>             Bases: <code>Dataset</code></p> <p>ALOSDataset</p> <p>The format is described in https://www.eorc.jaxa.jp/ALOS/en/alos-2/pdf/product_format_description/PALSAR-2_xx_Format_CEOS_E_g.pdf</p> <p>The dataset is constructed from the volume file. If leader and trailer files are colocated, they are loaded as well.</p> <p>Important, this code has been developed for working with L1.1 HBQ-R Quad Pol datafiles. It is not expected to work out of the box for other levels and for less than 4 polarizations.</p> <p>Parameters:</p> <ul> <li> <code>volpath</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>the path to the VOLUME file</p> </li> <li> <code>transform</code>         \u2013          <p>the transform applied the cropped image. It applies         on a dictionnary of patches {'HH': np.array, 'HV': np.array}</p> </li> <li> <code>crop_coordinates</code>             (<code>tuple</code>, default:                 <code>None</code> )         \u2013          <p>the subpart of the image to consider as ((row_i, col_i), (row_j, col_j))               defining the corner coordinates</p> </li> <li> <code>patch_size</code>             (<code>tuple</code>, default:                 <code>(128, 128)</code> )         \u2013          <p>the dimensions of the patches to consider (rows, cols)</p> </li> <li> <code>patch_stride</code>             (<code>tuple</code>, default:                 <code>None</code> )         \u2013          <p>the shift between two consecutive patches, default:patch_size</p> </li> </ul> Source code in <code>src/torchcvnn/datasets/alos2/dataset.py</code> <pre><code>def __init__(\n    self,\n    volpath: str = None,\n    transform=None,\n    crop_coordinates: tuple = None,\n    patch_size: tuple = (128, 128),\n    patch_stride: tuple = None,\n):\n    super().__init__()\n\n    self.transform = transform\n\n    self.patch_size = patch_size\n    self.patch_stride = patch_stride\n    if patch_stride is None:\n        self.patch_stride = patch_size\n\n    self.volFile = VolFile(volpath)\n\n    leader_filepath = volpath.parents[0] / volpath.name.replace(\"VOL-\", \"LED-\")\n    self.leaderFile = None\n    if leader_filepath.exists():\n        self.leaderFile = LeaderFile(leader_filepath)\n\n    trailer_filepath = volpath.parents[0] / volpath.name.replace(\"VOL-\", \"TRL-\")\n    self.trailerFile = None\n    if trailer_filepath.exists():\n        self.trailerFile = TrailerFile(trailer_filepath)\n\n    self.crop_coordinates = None\n    if crop_coordinates is not None:\n        self.crop_coordinates = crop_coordinates\n\n    self.images = {}\n    for pol in [\"HH\", \"HV\", \"VH\", \"VV\"]:\n        filepath = volpath.parents[0] / volpath.name.replace(\"VOL-\", f\"IMG-{pol}-\")\n        if not filepath.exists():\n            continue\n        self.images[pol] = SARImage(filepath)\n        if self.crop_coordinates is None:\n            self.crop_coordinates = (\n                (0, 0),\n                (self.images[pol].num_rows, self.images[pol].num_cols),\n            )\n\n    if len(self.images) != self.volFile.num_polarizations:\n        raise RuntimeError(\n            f\"I was expecting {self.volFile.num_polarizations} data file but I found {len(self.images)} data file\"\n        )\n\n    # Precompute the dimension of the grid of patches\n    nrows = self.crop_coordinates[1][0] - self.crop_coordinates[0][0]\n    ncols = self.crop_coordinates[1][1] - self.crop_coordinates[0][1]\n\n    nrows_patch, ncols_patch = self.patch_size\n    row_stride, col_stride = self.patch_stride\n\n    self.nsamples_per_rows = (nrows - nrows_patch) // row_stride + 1\n    self.nsamples_per_cols = (ncols - ncols_patch) // col_stride + 1\n</code></pre>"},{"location":"reference/torchcvnn/datasets/alos2/dataset/#torchcvnn.datasets.alos2.dataset.ALOSDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int)\n</code></pre> <p>Access and returns the subpatch specified by the index</p> <p>Parameters:</p> <ul> <li> <code>idx</code>             (<code>int</code>)         \u2013          <p>the index of the patch to access</p> </li> </ul> Source code in <code>src/torchcvnn/datasets/alos2/dataset.py</code> <pre><code>def __getitem__(self, idx: int):\n    \"\"\"\n    Access and returns the subpatch specified by the index\n\n    Arguments:\n        idx: the index of the patch to access\n    \"\"\"\n    row_stride, col_stride = self.patch_stride\n    start_row = (\n        self.crop_coordinates[0][0] + (idx // self.nsamples_per_cols) * row_stride\n    )\n    start_col = (\n        self.crop_coordinates[0][1] + (idx % self.nsamples_per_cols) * col_stride\n    )\n    num_rows, num_cols = self.patch_size\n    patches = {\n        pol: im.read_patch(start_row, num_rows, start_col, num_cols)\n        * self.leaderFile.calibration_factor\n        for pol, im in self.images.items()\n    }\n\n    if self.transform is not None:\n        patches = self.transform(patches)\n    else:\n        patches = np.stack([patchi for _, patchi in patches.items()])\n\n    return patches\n</code></pre>"},{"location":"reference/torchcvnn/datasets/alos2/dataset/#torchcvnn.datasets.alos2.dataset.ALOSDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the dataset according to the patch size, stride and image size</p> <p>Returns:</p> <ul> <li> <code>int</code> (            <code>int</code> )        \u2013          <p>the total number of available patches</p> </li> </ul> Source code in <code>src/torchcvnn/datasets/alos2/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset according to the patch size, stride\n    and image size\n\n    Returns:\n        int: the total number of available patches\n    \"\"\"\n\n    return self.nsamples_per_rows * self.nsamples_per_cols\n</code></pre>"},{"location":"reference/torchcvnn/datasets/alos2/leader_file/","title":"leader_file","text":""},{"location":"reference/torchcvnn/datasets/alos2/leader_file/#torchcvnn.datasets.alos2.leader_file.LeaderFile","title":"LeaderFile","text":"<pre><code>LeaderFile(filepath)\n</code></pre> <p>Processing of the SAR trailer file.</p> Source code in <code>src/torchcvnn/datasets/alos2/leader_file.py</code> <pre><code>def __init__(self, filepath):\n    self.descriptor_record = {}\n    self.summary_record = {}\n    self.altitude_record = {}\n    self.radiometric_record = {}\n    with open(filepath, \"rb\") as fh:\n        fh_offset = 0\n        fh_offset = parse_utils.parse_from_format(\n            fh,\n            self.descriptor_record,\n            descriptor_format,\n            1,\n            descriptor_record_length,\n            fh_offset,\n        )\n        fh_offset = parse_utils.parse_from_format(\n            fh,\n            self.summary_record,\n            summary_format,\n            1,\n            summary_record_length,\n            fh_offset,\n        )\n\n        # Skip map projection record\n        # Does not exist in L1.1\n        if self.descriptor_record[\"number_map_projection_record\"] == 1:\n            fh_offset += map_projection_record_length\n\n        # Skip platform projection record\n        fh_offset += platform_projection_record_length\n\n        # Altitude data record\n        fh_offset = parse_utils.parse_from_format(\n            fh,\n            self.altitude_record,\n            altitude_data_format,\n            1,\n            altitude_data_record_length,\n            fh_offset,\n        )\n\n        # Radiometric data record\n        fh_offset = parse_utils.parse_from_format(\n            fh,\n            self.radiometric_record,\n            radiometric_data_format,\n            1,\n            radiometric_data_record_length,\n            fh_offset,\n        )\n</code></pre>"},{"location":"reference/torchcvnn/datasets/alos2/parse_utils/","title":"parse_utils","text":""},{"location":"reference/torchcvnn/datasets/alos2/sar_image/","title":"sar_image","text":""},{"location":"reference/torchcvnn/datasets/alos2/sar_image/#torchcvnn.datasets.alos2.sar_image.SARImage","title":"SARImage","text":"<pre><code>SARImage(filepath)\n</code></pre> <p>Processing of the SAR Image</p> Source code in <code>src/torchcvnn/datasets/alos2/sar_image.py</code> <pre><code>def __init__(self, filepath):\n    self.descriptor_records = {}\n    self.data_records = {}\n    self.filepath = filepath\n\n    with open(filepath, \"rb\") as fh:\n        fh_offset = 0\n        fh_offset = parse_utils.parse_from_format(\n            fh,\n            self.descriptor_records,\n            descriptor_format,\n            1,\n            descriptor_record_length,\n            fh_offset,\n        )\n\n        # Read the header of the first record\n        fh_offset = parse_utils.parse_from_format(\n            fh,\n            self.data_records,\n            data_records_format,\n            1,\n            data_record_header_length,\n            fh_offset,\n        )\n</code></pre>"},{"location":"reference/torchcvnn/datasets/alos2/trailer_file/","title":"trailer_file","text":""},{"location":"reference/torchcvnn/datasets/alos2/trailer_file/#torchcvnn.datasets.alos2.trailer_file.TrailerFile","title":"TrailerFile","text":"<pre><code>TrailerFile(filepath)\n</code></pre> <p>Processing of the SAR trailer file.</p> Source code in <code>src/torchcvnn/datasets/alos2/trailer_file.py</code> <pre><code>def __init__(self, filepath):\n    self.descriptor_records = {}\n    with open(filepath, \"rb\") as fh:\n        fh_offset = 0\n        fh_offset = parse_utils.parse_from_format(\n            fh,\n            self.descriptor_records,\n            descriptor_format,\n            1,\n            descriptor_record_length,\n            fh_offset,\n        )\n\n        # Move the reading head to the beginning of the buffer\n        fh.seek(fh_offset)\n        num_pixels = self.descriptor_records[\"number_of_pixels\"]\n        num_lines = self.descriptor_records[\"number_of_lines\"]\n        self.image_data = parse_image_data(fh, num_pixels, num_lines)\n</code></pre>"},{"location":"reference/torchcvnn/datasets/alos2/vol_file/","title":"vol_file","text":""},{"location":"reference/torchcvnn/datasets/alos2/vol_file/#torchcvnn.datasets.alos2.vol_file.VolFile","title":"VolFile","text":"<pre><code>VolFile(filepath: Union[str, Path])\n</code></pre> <p>Processing a Volume Directory file in the CEOS format. The parsed informations can be accessed through the attributes <code>descriptor_records</code>, <code>file_pointer_records</code> and <code>text_records</code></p> <p>Parameters:</p> <ul> <li> <code>filepath</code>             (<code>Union[str, Path]</code>)         \u2013          <p>the path to the volume directory file</p> </li> </ul> Source code in <code>src/torchcvnn/datasets/alos2/vol_file.py</code> <pre><code>def __init__(self, filepath: Union[str, pathlib.Path]):\n    self.descriptor_records = {}\n    self.file_pointer_records = []\n    self.text_record = {}\n\n    with open(filepath, \"rb\") as fh:\n        # Parsing the volume descriptor\n        fh_offset = 0\n        fh_offset = parse_utils.parse_from_format(\n            fh,\n            self.descriptor_records,\n            descriptor_format,\n            1,\n            volume_descriptor_record_length,\n            fh_offset,\n        )\n\n        # Parsing the file pointer\n        number_of_file_pointer_records = self.descriptor_records[\n            \"number_of_file_pointer_records\"\n        ]\n        fh_offset = parse_utils.parse_from_format(\n            fh,\n            self.file_pointer_records,\n            file_pointer_format,\n            number_of_file_pointer_records,\n            file_pointer_record_length,\n            fh_offset,\n        )\n\n        # Parsing the file pointer\n        fh_offset = parse_utils.parse_from_format(\n            fh,\n            self.text_record,\n            text_records_format,\n            1,\n            text_record_length,\n            fh_offset,\n        )\n</code></pre>"},{"location":"reference/torchcvnn/datasets/bretigny/","title":"Index","text":""},{"location":"reference/torchcvnn/datasets/bretigny/dataset/","title":"dataset","text":""},{"location":"reference/torchcvnn/datasets/bretigny/dataset/#torchcvnn.datasets.bretigny.dataset.Bretigny","title":"Bretigny","text":"<pre><code>Bretigny(\n    root: str,\n    fold: str,\n    transform=None,\n    balanced: bool = False,\n    patch_size: tuple = (128, 128),\n    patch_stride: tuple = None,\n)\n</code></pre> <p>             Bases: <code>Dataset</code></p> <p>Bretigny Dataset</p> <p>Parameters:</p> <ul> <li> <code>root</code>             (<code>str</code>)         \u2013          <p>the root directory containing the npz files for Bretigny</p> </li> <li> <code>fold</code>             (<code>str</code>)         \u2013          <p>train (70%), valid (15%), or test (15%)</p> </li> <li> <code>transform</code>         \u2013          <p>the transform applied the cropped image</p> </li> <li> <code>balanced</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>whether or not to use balanced labels</p> </li> <li> <code>patch_size</code>             (<code>tuple</code>, default:                 <code>(128, 128)</code> )         \u2013          <p>the dimensions of the patches to consider (rows, cols)</p> </li> <li> <code>patch_stride</code>             (<code>tuple</code>, default:                 <code>None</code> )         \u2013          <p>the shift between two consecutive patches, default:patch_size</p> </li> </ul> Note <p>An example usage :</p> <pre><code>import torchcvnn\nfrom torchcvnn.datasets import Bretigny\n\ndataset = Bretigny(\n    rootdir, fold=\"train\", patch_size=((128, 128)), transform=lambda x: np.abs(x)\n)\nX, y = dataset[0]\n</code></pre> <p>Displayed below are the train, valid and test parts with the labels overlayed</p> <p> </p> Source code in <code>src/torchcvnn/datasets/bretigny/dataset.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    fold: str,\n    transform=None,\n    balanced: bool = False,\n    patch_size: tuple = (128, 128),\n    patch_stride: tuple = None,\n):\n    self.root = pathlib.Path(root)\n    self.transform = transform\n\n    self.patch_size = patch_size\n    self.patch_stride = patch_stride\n    if patch_stride is None:\n        self.patch_stride = patch_size\n\n    # Preload the data\n    sar_filename = self.root / \"bretigny_seg.npz\"\n    if not sar_filename.exists():\n        raise RuntimeError(f\"Cannot find the file {sar_filename}\")\n    sar_data = np.load(sar_filename)\n    self.HH, self.HV, self.VV = sar_data[\"HH\"], sar_data[\"HV\"], sar_data[\"VV\"]\n\n    if balanced:\n        label_filename = self.root / \"bretigny_seg_4ROI_balanced.npz\"\n    else:\n        label_filename = self.root / \"bretigny_seg_4ROI.npz\"\n    if not label_filename.exists():\n        raise RuntimeError(f\"Cannot find the label file {label_filename}\")\n    self.labels = np.load(label_filename)[\"arr_0\"]\n\n    if not fold in [\"train\", \"valid\", \"test\"]:\n        raise ValueError(\n            f\"Unrecognized fold {fold}. Should be either train, valid or test\"\n        )\n\n    # Crop the data with respect to the fold\n    if fold == \"train\":\n        col_start = 0\n        col_end = int(0.70 * self.HH.shape[1])\n    elif fold == \"valid\":\n        col_start = int(0.70 * self.HH.shape[1]) + 1\n        col_end = int(0.85 * self.HH.shape[1])\n    else:\n        col_start = int(0.85 * self.HH.shape[1]) + 1\n        col_end = self.HH.shape[1]\n\n    self.HH = self.HH[:, col_start:col_end]\n    self.HV = self.HV[:, col_start:col_end]\n    self.VV = self.VV[:, col_start:col_end]\n    self.labels = self.labels[:, col_start:col_end]\n\n    # Precompute the dimension of the grid of patches\n    nrows = self.HH.shape[0]\n    ncols = self.HH.shape[1]\n\n    nrows_patch, ncols_patch = self.patch_size\n    row_stride, col_stride = self.patch_stride\n\n    self.nsamples_per_rows = (nrows - nrows_patch) // row_stride + 1\n    self.nsamples_per_cols = (ncols - ncols_patch) // col_stride + 1\n</code></pre>"},{"location":"reference/torchcvnn/datasets/bretigny/dataset/#torchcvnn.datasets.bretigny.dataset.Bretigny.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx) -&gt; Tuple[Any, Any]\n</code></pre> <p>Returns the indexes patch.</p> <p>Parameters:</p> <ul> <li> <code>idx</code>             (<code>int</code>)         \u2013          <p>Index</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code> (            <code>Tuple[Any, Any]</code> )        \u2013          <p>(patch, labels) where patch contains the 3 complex valued polarization HH, HV, VV and labels contains the aligned semantic labels</p> </li> </ul> Source code in <code>src/torchcvnn/datasets/bretigny/dataset.py</code> <pre><code>def __getitem__(self, idx) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Returns the indexes patch.\n\n    Arguments:\n        idx (int): Index\n\n    Returns:\n        tuple: (patch, labels) where patch contains the 3 complex valued polarization HH, HV, VV and labels contains the aligned semantic labels\n    \"\"\"\n    row_stride, col_stride = self.patch_stride\n    start_row = (idx // self.nsamples_per_cols) * row_stride\n    start_col = (idx % self.nsamples_per_cols) * col_stride\n    num_rows, num_cols = self.patch_size\n    patches = [\n        patch[\n            start_row : (start_row + num_rows), start_col : (start_col + num_cols)\n        ]\n        for patch in [self.HH, self.HV, self.VV]\n    ]\n    patches = np.stack(patches)\n    if self.transform is not None:\n        patches = self.transform(patches)\n\n    labels = self.labels[\n        start_row : (start_row + num_rows), start_col : (start_col + num_cols)\n    ]\n\n    return patches, labels\n</code></pre>"},{"location":"reference/torchcvnn/datasets/bretigny/dataset/#torchcvnn.datasets.bretigny.dataset.Bretigny.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the total number of patches in the whole image.</p> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013          <p>the total number of patches in the dataset</p> </li> </ul> Source code in <code>src/torchcvnn/datasets/bretigny/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the total number of patches in the whole image.\n\n    Returns:\n        the total number of patches in the dataset\n    \"\"\"\n    return self.nsamples_per_rows * self.nsamples_per_cols\n</code></pre>"},{"location":"reference/torchcvnn/datasets/slc/","title":"Index","text":"<p>Scripts to read and parse SLC data files</p>"},{"location":"reference/torchcvnn/datasets/slc/ann_file/","title":"ann_file","text":""},{"location":"reference/torchcvnn/datasets/slc/ann_file/#torchcvnn.datasets.slc.ann_file.AnnFile","title":"AnnFile","text":"<pre><code>AnnFile(filename)\n</code></pre> <p>From the documentation :</p> <p>The annotation file (.ann) is a keyword/value ASCII file in which the value on the right of the equals sign corresponds to the keyword on the left of the equals sign. The number of keywords may change with time, so the line number should not be assumed to be constant for any given keyword.</p> <p>In addition, the spacing between keywords and values may change. The units are given in parenthesis between the keyword and equal sign, and may change from annotation file to annotation file and within each annotation file.</p> <p>Comments are indicated by semicolons (;), and may occur at the beginning of a line, or at the middle of a line (everything after the semicolon on that line is a comment). The length of each text line is variable, and ends with a carriage return. There may be lines with just a carriage return or spaces and a carriage return.</p> Source code in <code>src/torchcvnn/datasets/slc/ann_file.py</code> <pre><code>def __init__(self, filename):\n    self.filename = filename\n    self.parameters = []\n    self.data = self.read()\n</code></pre>"},{"location":"reference/torchcvnn/datasets/slc/dataset/","title":"dataset","text":""},{"location":"reference/torchcvnn/datasets/slc/dataset/#torchcvnn.datasets.slc.dataset.SLCDataset","title":"SLCDataset","text":"<pre><code>SLCDataset(\n    rootdir: str = None,\n    transform=None,\n    patch_size: tuple = (128, 128),\n    patch_stride: tuple = None,\n)\n</code></pre> <p>             Bases: <code>Dataset</code></p> <p>SLCDataset</p> <p>The format is described in https://uavsar.jpl.nasa.gov/science/documents/stack-format.html</p> <p>This object does not download the data for you, you must have the data on your local machine. For example, you can register and access data from the NASA JetLab https://uavsar.jpl.nasa.gov</p> <p>Note the datafiles can be quite large. For example, the quad polarization from Los Angeles SSurge_15305 is a bit more than 30 GB. If you take the downsampled datasets 2x8, it is 2GB.</p> <p>Note the 1x1 is 1.67 m slant range x 0.6 m azimuth.</p> Note <p>As an example, using the example <code>read_slc.py</code>, with the SSurge_15305 stack provided by the UAVSar, the Pauli representation of the four polarizations is shown below :</p> <p></p> <p>The code may look like this :</p> <pre><code>import numpy as np\nimport torchcvnn\nfrom torchcvnn.datasets.slc.dataset import SLCDataset\n\ndef get_pauli(data):\n    # Returns Pauli in (H, W, C)\n    HH = data[\"HH\"]\n    HV = data[\"HV\"]\n    VH = data[\"VH\"]\n    VV = data[\"VV\"]\n\n    alpha = HH + VV\n    beta = HH - VV\n    gamma = HV + VH\n\n    return np.stack([beta, gamma, alpha], axis=-1)\n\n\npatch_size = (3000, 3000)\ndataset = SLCDataset(\n    rootdir,\n    transform=get_pauli,\n    patch_size=patch_size,\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>rootdir</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>the path containing the SLC and ANN files</p> </li> <li> <code>transform</code>         \u2013          <p>the transform applied to the patches. It applies         on a dictionnary of patches {'HH': np.array, 'HV': np.array, ...}</p> </li> <li> <code>patch_size</code>             (<code>tuple</code>, default:                 <code>(128, 128)</code> )         \u2013          <p>the dimensions of the patches to consider (rows, cols)</p> </li> <li> <code>patch_stride</code>             (<code>tuple</code>, default:                 <code>None</code> )         \u2013          <p>the shift between two consecutive patches, default:patch_size</p> </li> </ul> Source code in <code>src/torchcvnn/datasets/slc/dataset.py</code> <pre><code>def __init__(\n    self,\n    rootdir: str = None,\n    transform=None,\n    patch_size: tuple = (128, 128),\n    patch_stride: tuple = None,\n):\n    super().__init__()\n\n    self.transform = transform\n    self.patch_size = patch_size\n    self.patch_stride = patch_stride\n    if self.patch_stride is None:\n        self.patch_stride = patch_size\n\n    # Let us find all the SLC files\n    # We group the polorizations of the stack together\n    self.slcs = glob.glob(str(pathlib.Path(rootdir) / \"*.slc\"))\n    self.slc_polarizations = {}\n    self.patch_counts = {}\n    for slc in self.slcs:\n        slc_file = SLCFile(slc, patch_size=patch_size, patch_stride=patch_stride)\n        slc_key = slc_file.key\n        if slc_key not in self.slc_polarizations:\n            self.slc_polarizations[slc_key] = {}\n\n        self.slc_polarizations[slc_key][slc_file.polarization] = slc_file\n        self.patch_counts[slc_key] = len(slc_file)\n\n    # Sanity checks\n    # 1- For every SLC stack, we must have the same number of patches\n    # 2- All the SLCs must have the same number of polarizations\n    polarization_count = None\n    for slc_key, slc_polarizations in self.slc_polarizations.items():\n        if polarization_count is None:\n            polarization_count = len(slc_polarizations)\n        else:\n            assert polarization_count == len(slc_polarizations)\n\n        patch_count = None\n        for polarization, slc_file in slc_polarizations.items():\n            if patch_count is None:\n                patch_count = len(slc_file)\n            else:\n                assert patch_count == len(slc_file)\n\n    self.nsamples = sum(self.patch_counts.values())\n</code></pre>"},{"location":"reference/torchcvnn/datasets/slc/dataset/#torchcvnn.datasets.slc.dataset.SLCDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(item)\n</code></pre> <p>Usefull params : - 1x1_slc_azimuth_pixel_spacing - 1x1_slc_range_pixel_spacing - global_average_squint_angle - center_wavelength - slc_SEGMENT_1x1_rows - slc_SEGMENT_1x1_columns</p> Source code in <code>src/torchcvnn/datasets/slc/dataset.py</code> <pre><code>def __getitem__(self, item):\n    \"\"\"\n    Usefull params :\n    - 1x1_slc_azimuth_pixel_spacing\n    - 1x1_slc_range_pixel_spacing\n    - global_average_squint_angle\n    - center_wavelength\n    - slc_SEGMENT_1x1_rows\n    - slc_SEGMENT_1x1_columns\n    \"\"\"\n    # Now, request the patch from the right SLC file\n    assert 0 &lt;= item &lt; self.nsamples\n\n    # 1- Find the right SLC file given self.patch_counts and item index\n    for slc_key, count in self.patch_counts.items():\n        if item &lt; count:\n            slcs = self.slc_polarizations[slc_key]\n            break\n        else:\n            item -= count\n    sorted_keys = sorted(slcs.keys())\n    # 2- Find the right patch from all the polarizations\n    patches = {pol: slcs[pol][item] for pol in sorted_keys}\n\n    # 3a- Stack the patches\n    # 3b- Apply the transform\n    if self.transform is not None:\n        patches = self.transform(patches)\n    else:\n        patches = np.stack([patchi for _, patchi in patches.items()])\n\n    return patches\n</code></pre>"},{"location":"reference/torchcvnn/datasets/slc/slc_file/","title":"slc_file","text":""},{"location":"reference/torchcvnn/datasets/slc/slc_file/#torchcvnn.datasets.slc.slc_file.SLCFile","title":"SLCFile","text":"<pre><code>SLCFile(\n    filename: str,\n    patch_size: tuple,\n    patch_stride: tuple = None,\n)\n</code></pre> <p>Reads a SLC file</p> <p>The filenames contain interesting information:</p> <p>{site name}{line ID}{data take counter}{band}{steering}{polarization}... {baseline correction}.slc}_{downsample factor</p> <p>e.g. SSurge_15305_14170_007_141120_L090HH_01_BC_s1_1x1.slc is</p> <ul> <li>site_name : SSurge</li> <li>line ID : 15305</li> <li>flight ID : 14170</li> <li>data take counter : 007</li> <li>acquisition date : 141120, the date is in YYMMDD format (UTC time).</li> <li>band : L</li> <li>steering : 090</li> <li>polarization : HH</li> <li>stack version : 01</li> <li>baseline correction : BC, means the data is corrected for residual baseline</li> <li>segment number : s1</li> <li>downsample factor : 1x1</li> </ul> <p>There is one SLC file per segment and per polarization.</p> Source code in <code>src/torchcvnn/datasets/slc/slc_file.py</code> <pre><code>def __init__(self, filename: str, patch_size: tuple, patch_stride: tuple = None):\n    self.filename = pathlib.Path(filename)\n    self.parameters = parse_slc_filename(self.filename.name)\n    self.patch_size = patch_size\n    self.patch_stride = patch_stride\n    if self.patch_stride is None:\n        self.patch_stride = patch_size\n\n    # The annotation filename is almost the same as the SLC filename, except we drop\n    # the segment number and downsample factor\n    # We expect it to be colocated with the SLC file\n    ann_filename = \"_\".join(str(self.filename.name).split(\"_\")[:-2]) + \".ann\"\n    self.ann_file = AnnFile(str(self.filename.parent / ann_filename))\n\n    downsample_factor = self.parameters[\"downsample_factor\"]\n    segment_number = self.parameters[\"segment_number\"]\n    # self.azimuth_pixel_spacing = getattr(\n    #     self.ann_file, f\"{downsample_factor}_slc_azimuth_pixel_spacing\"\n    # )\n    # self.range_pixel_spacing = getattr(\n    #     self.ann_file, f\"{downsample_factor}_slc_range_pixel_spacing\"\n    # )\n    # self.global_average_squint_angle = self.ann_file.global_average_squint_angle\n    # self.center_wavelength = self.ann_file.center_wavelength\n    self.n_rows = getattr(\n        self.ann_file, f\"slc_{segment_number}_{downsample_factor}_rows\"\n    )\n    self.n_cols = getattr(\n        self.ann_file, f\"slc_{segment_number}_{downsample_factor}_columns\"\n    )\n\n    # Precompute the dimension of the grid of patches\n    nrows_patch, ncols_patch = self.patch_size\n    row_stride, col_stride = self.patch_stride\n\n    self.nsamples_per_rows = (self.n_rows - nrows_patch) // row_stride + 1\n    self.nsamples_per_cols = (self.n_cols - ncols_patch) // col_stride + 1\n</code></pre>"},{"location":"reference/torchcvnn/datasets/slc/slc_file/#torchcvnn.datasets.slc.slc_file.SLCFile.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(item)\n</code></pre> <p>Returns the item-th patch from the SLC file</p> Source code in <code>src/torchcvnn/datasets/slc/slc_file.py</code> <pre><code>def __getitem__(self, item):\n    \"\"\"\n    Returns the item-th patch from the SLC file\n    \"\"\"\n    assert 0 &lt;= item &lt; len(self)\n    # Compute the row and column index of the patch\n    row = item // self.nsamples_per_cols\n    col = item % self.nsamples_per_cols\n\n    # Compute the starting row and column index of the patch\n    row_stride, col_stride = self.patch_stride\n    row_start = row * row_stride\n    col_start = col * col_stride\n\n    # Read the patch\n    # The SLC file is a binary file of complex64\n    patch = np.zeros(self.patch_size, dtype=np.complex64)\n    with open(self.filename, \"rb\") as fh:\n        for row in range(self.patch_size[0]):\n            fh.seek(\n                (row_start + row) * self.n_cols * 8 + col_start * 8, os.SEEK_SET\n            )\n            patch[row, :] = np.fromfile(\n                fh, dtype=np.complex64, count=self.patch_size[1]\n            )\n    return patch\n</code></pre>"},{"location":"reference/torchcvnn/datasets/slc/slc_file/#torchcvnn.datasets.slc.slc_file.SLCFile.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Returns the number of patches that can be extracted from the SLC file</p> Source code in <code>src/torchcvnn/datasets/slc/slc_file.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Returns the number of patches that can be extracted from the SLC file\n    \"\"\"\n    return self.nsamples_per_rows * self.nsamples_per_cols\n</code></pre>"},{"location":"reference/torchcvnn/datasets/slc/slc_file/#torchcvnn.datasets.slc.slc_file.parse_slc_filename","title":"parse_slc_filename","text":"<pre><code>parse_slc_filename(filename)\n</code></pre> <p>Parses a filename of a SLC file {site name}{line ID}{data take counter}{band}{steering}{polarization}... {baseline correction}.slc}_{downsample factor</p> <p>and returns all the information in a dictionary</p> Source code in <code>src/torchcvnn/datasets/slc/slc_file.py</code> <pre><code>def parse_slc_filename(filename):\n    \"\"\"\n    Parses a filename of a SLC file\n    {site name}_{line ID}_{flight ID}_{data take counter}_{acquisition date}_{band}{steering}{polarization}_{stack_version}... _{baseline correction}_{segment number}_{downsample factor}.slc\n\n    and returns all the information in a dictionary\n    \"\"\"\n    # Remove the .slc extension and split the fields\n    fields = filename[:-4].split(\"_\")\n    parameters = {\n        \"site_name\": fields[0],\n        \"line_ID\": fields[1],\n        \"flight_ID\": fields[2],\n        \"data_take_counter\": fields[3],\n        \"acquisition_date\": fields[4],\n        \"band\": fields[5][0],\n        \"steering\": fields[5][1:-2],\n        \"polarization\": fields[5][-2:],\n        \"stack_version\": fields[6],\n        \"baseline_correction\": fields[7],\n        \"segment_number\": int(\n            fields[8][1:]\n        ),  # the segment is encoded as  s{segment_number}\n        \"downsample_factor\": fields[9],\n    }\n    return parameters\n</code></pre>"},{"location":"reference/torchcvnn/nn/","title":"Index","text":""},{"location":"reference/torchcvnn/nn/modules/","title":"Index","text":""},{"location":"reference/torchcvnn/nn/modules/activation/","title":"activation","text":""},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CCELU","title":"CCELU","text":"<pre><code>CCELU()\n</code></pre> <p>             Bases: <code>IndependentRealImag</code></p> <p>Applies a CELU independently on both the real and imaginary parts</p> \\[ CCELU(z) = CELU(Re[z]) + CELU(Im[z])j \\] Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__(nn.CELU)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CCELU.forward","title":"forward","text":"<pre><code>forward(z: tensor) -&gt; tensor\n</code></pre> <p>Performs the forward pass</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.tensor) -&gt; torch.tensor:\n    \"\"\"\n    Performs the forward pass\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    return self.act_real(z.real) + self.act_imag(z.imag) * 1j\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CELU","title":"CELU","text":"<pre><code>CELU()\n</code></pre> <p>             Bases: <code>IndependentRealImag</code></p> <p>Applies a ELU independently on both the real and imaginary parts</p> <p>Not to confuse with <code>torch.nn.CELU</code>. For the complex equivalent of <code>torch.nn.CELU</code>, see torchcvnn.nn.modules.activation.CCELU</p> \\[ CELU(z) = ELU(Re[z]) + ELU(Im[z])j \\] Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__(nn.ELU)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CELU.forward","title":"forward","text":"<pre><code>forward(z: tensor) -&gt; tensor\n</code></pre> <p>Performs the forward pass</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.tensor) -&gt; torch.tensor:\n    \"\"\"\n    Performs the forward pass\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    return self.act_real(z.real) + self.act_imag(z.imag) * 1j\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CGELU","title":"CGELU","text":"<pre><code>CGELU()\n</code></pre> <p>             Bases: <code>IndependentRealImag</code></p> <p>Applies a GELU independently on both the real and imaginary parts</p> \\[ CGELU(z) = GELU(Re[z]) + GELU(Im[z])j \\] Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__(nn.GELU)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CGELU.forward","title":"forward","text":"<pre><code>forward(z: tensor) -&gt; tensor\n</code></pre> <p>Performs the forward pass</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.tensor) -&gt; torch.tensor:\n    \"\"\"\n    Performs the forward pass\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    return self.act_real(z.real) + self.act_imag(z.imag) * 1j\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CPReLU","title":"CPReLU","text":"<pre><code>CPReLU()\n</code></pre> <p>             Bases: <code>IndependentRealImag</code></p> <p>Applies a PReLU independently on both the real and imaginary parts</p> \\[ CPReLU(z) = PReLU(Re[z]) + PReLU(Im[z])j \\] Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__(nn.PReLU)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CPReLU.forward","title":"forward","text":"<pre><code>forward(z: tensor) -&gt; tensor\n</code></pre> <p>Performs the forward pass</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.tensor) -&gt; torch.tensor:\n    \"\"\"\n    Performs the forward pass\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    return self.act_real(z.real) + self.act_imag(z.imag) * 1j\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CReLU","title":"CReLU","text":"<pre><code>CReLU()\n</code></pre> <p>             Bases: <code>IndependentRealImag</code></p> <p>Applies a ReLU independently on both the real and imaginary parts</p> \\[ CReLU(z) = ReLU(Re[z]) + ReLU(Im[z])j \\] <p>Only the quadrant where both <code>Re[z]</code> and <code>Im[z]</code> are negative is projected to \\(0\\). Otherwise either the real and/or the imaginary part is preserved.</p> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__(nn.ReLU)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CReLU.forward","title":"forward","text":"<pre><code>forward(z: tensor) -&gt; tensor\n</code></pre> <p>Performs the forward pass</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.tensor) -&gt; torch.tensor:\n    \"\"\"\n    Performs the forward pass\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    return self.act_real(z.real) + self.act_imag(z.imag) * 1j\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CSigmoid","title":"CSigmoid","text":"<pre><code>CSigmoid()\n</code></pre> <p>             Bases: <code>IndependentRealImag</code></p> <p>Applies a Sigmoid independently on both the real and imaginary parts</p> <p>as used in Nitta Tohru. An extension of the back-propagation algorithm to complex numbers. Neural Networks, 10(9):1391\u20131415, November 1997.</p> \\[ CSigmoid(z) = Sigmoid(Re[z]) + Sigmoid(Im[z])j \\] <p>where the real valued sigmoid is applied in the right hand side terms.</p> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__(nn.Sigmoid)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CSigmoid.forward","title":"forward","text":"<pre><code>forward(z: tensor) -&gt; tensor\n</code></pre> <p>Performs the forward pass</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.tensor) -&gt; torch.tensor:\n    \"\"\"\n    Performs the forward pass\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    return self.act_real(z.real) + self.act_imag(z.imag) * 1j\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CTanh","title":"CTanh","text":"<pre><code>CTanh()\n</code></pre> <p>             Bases: <code>IndependentRealImag</code></p> <p>Applies a Tanh independently on both the real and imaginary parts</p> \\[ CTanh(z) = Tanh(Re[z]) + Tanh(Im[z])j \\] <p>where the real valued sigmoid is applied in the right hand side terms.</p> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__(nn.Tanh)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.CTanh.forward","title":"forward","text":"<pre><code>forward(z: tensor) -&gt; tensor\n</code></pre> <p>Performs the forward pass</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.tensor) -&gt; torch.tensor:\n    \"\"\"\n    Performs the forward pass\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    return self.act_real(z.real) + self.act_imag(z.imag) * 1j\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.Cardioid","title":"Cardioid","text":"<pre><code>Cardioid()\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Applies a ReLU with parametric offset on the amplitude, keeping the phase unchanged.</p> \\[ Cardioid(z = r e^{\\theta j}) = \\frac{1+\\cos(\\theta)}{2} z \\] <p>As proposed by Virtue et al. (2019). For real numbers, e.g. \\(\\theta \\in \\{0, \\pi\\}\\), it reduces to the ReLU :</p> \\[ \\forall r \\in \\mathbb{R}, \\theta \\in \\{0, \\pi\\}, Cardiod(r e^{\\theta j}) = Relu(r) \\] Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.b = torch.nn.Parameter(torch.tensor(0.0, dtype=torch.float), True)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.Cardioid.forward","title":"forward","text":"<pre><code>forward(z: Tensor)\n</code></pre> <p>Performs the forward pass.</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>Tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.Tensor):\n    \"\"\"\n    Performs the forward pass.\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    return 0.5 * (1 + torch.cos(z.angle())) * z\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.IndependentRealImag","title":"IndependentRealImag","text":"<pre><code>IndependentRealImag(fact: Module)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Generic module to apply a real valued activation function independently on both the real and imaginary part</p> <p>Parameters:</p> <ul> <li> <code>fact</code>             (<code>Module</code>)         \u2013          <p>A nn.Module name of a real valued activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self, fact: nn.Module):\n    super().__init__()\n    self.act_real = fact()\n    self.act_imag = fact()\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.IndependentRealImag.forward","title":"forward","text":"<pre><code>forward(z: tensor) -&gt; tensor\n</code></pre> <p>Performs the forward pass</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.tensor) -&gt; torch.tensor:\n    \"\"\"\n    Performs the forward pass\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    return self.act_real(z.real) + self.act_imag(z.imag) * 1j\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.Mod","title":"Mod","text":"<pre><code>Mod()\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Extracts the magnitude of the complex input: maps to \\(\\mathbb{R}\\)</p> \\[ Mod(z) = |z| \\] <p>This activation function allows to go from complex values to real values.</p> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.Mod.forward","title":"forward","text":"<pre><code>forward(z: Tensor)\n</code></pre> <p>Performs the forward pass.</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>Tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.Tensor):\n    \"\"\"\n    Performs the forward pass.\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    return torch.abs(z)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.modReLU","title":"modReLU","text":"<pre><code>modReLU()\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Applies a ReLU with parametric offset on the amplitude, keeping the phase unchanged.</p> \\[ modReLU(z = r e^{\\theta j}) = ReLU(r + b) e^{\\theta j}) \\] Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.b = torch.nn.Parameter(torch.tensor(0.0, dtype=torch.float), True)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.modReLU.forward","title":"forward","text":"<pre><code>forward(z: Tensor)\n</code></pre> <p>Performs the forward pass.</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>Tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.Tensor):\n    \"\"\"\n    Performs the forward pass.\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    return nn.functional.relu(z.abs() + self.b) * torch.exp(1j * z.angle())\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.zAbsReLU","title":"zAbsReLU","text":"<pre><code>zAbsReLU()\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Applies a zAbsReLU</p> \\[ zAbsReLU(z) = \\begin{cases} z &amp; \\mbox{if } |z| \\geq a\\\\ 0 &amp; \\mbox{otherwise}  \\end{cases} \\] <p>This cancels all the complex plane in the circle of radius \\(a\\), where \\(a\\) is trainable.</p> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.a = torch.nn.parameter.Parameter(\n        data=torch.Tensor([1.0]), requires_grad=True\n    )\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.zAbsReLU.forward","title":"forward","text":"<pre><code>forward(z: Tensor)\n</code></pre> <p>Performs the forward pass.</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>Tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.Tensor):\n    \"\"\"\n    Performs the forward pass.\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    mask = z.abs() &lt; self.a\n    return z * mask\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.zLeakyReLU","title":"zLeakyReLU","text":"<pre><code>zLeakyReLU()\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Applies a zReLU</p> \\[ zLeajyReLU(z) = \\begin{cases} z &amp; \\mbox{if } Re[z] &gt; 0 \\mbox{ and } Im[z] &gt; 0\\\\ a.z &amp; \\mbox{otherwise}  \\end{cases} \\] Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.a = torch.nn.parameter.Parameter(\n        data=torch.Tensor([0.2]), requires_grad=True\n    )\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.zLeakyReLU.forward","title":"forward","text":"<pre><code>forward(z: Tensor)\n</code></pre> <p>Performs the forward pass.</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>Tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.Tensor):\n    \"\"\"\n    Performs the forward pass.\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    pos_real = z.real &gt; 0\n    pos_img = z.imag &gt; 0\n    return z * pos_real * pos_img + self.a * (z * ~(pos_real * pos_img))\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.zReLU","title":"zReLU","text":"<pre><code>zReLU()\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Applies a zReLU</p> \\[ zRELU(z) = \\begin{cases} z &amp; \\mbox{if } Re[z] &gt; 0 \\mbox{ and } Im[z] &gt; 0\\\\ 0 &amp; \\mbox{otherwise}  \\end{cases} \\] <p>All the quadrant where both <code>Re[z]</code> and <code>Im[z]</code> are non negative are projected to \\(0\\). In other words, only one quadrant is preserved.</p> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/activation/#torchcvnn.nn.modules.activation.zReLU.forward","title":"forward","text":"<pre><code>forward(z: Tensor)\n</code></pre> <p>Performs the forward pass.</p> <p>Parameters:</p> <ul> <li> <code>z</code>             (<code>Tensor</code>)         \u2013          <p>the input tensor on which to apply the activation function</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/activation.py</code> <pre><code>def forward(self, z: torch.Tensor):\n    \"\"\"\n    Performs the forward pass.\n\n    Arguments:\n        z: the input tensor on which to apply the activation function\n    \"\"\"\n    pos_real = z.real &gt; 0\n    pos_img = z.imag &gt; 0\n    return z * pos_real * pos_img\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/batchnorm/","title":"batchnorm","text":""},{"location":"reference/torchcvnn/nn/modules/batchnorm/#torchcvnn.nn.modules.batchnorm.BatchNorm1d","title":"BatchNorm1d","text":"<pre><code>BatchNorm1d(\n    num_features: int,\n    eps: float = 1e-05,\n    momentum: float = 0.1,\n    affine: bool = True,\n    track_running_stats: bool = True,\n    device: device = None,\n    cdtype: dtype = torch.complex64,\n)\n</code></pre> <p>             Bases: <code>_BatchNormNd</code></p> <p>BatchNorm for complex valued neural networks. The same code applies for BatchNorm1d, BatchNorm2d, the only condition being the input tensor must be (batch_size, features, d1, d2, ..)</p> <p>The statistics will be computed over the \\(batch\\_size \\times d_1 \\times d_2 \\times ..\\) vectors of size \\(features\\).</p> <p>As defined by Trabelsi et al. (2018)</p> <p>Parameters:</p> <ul> <li> <code>num_features</code>             (<code>int</code>)         \u2013          <p>\\(C\\) from an expected input of size \\((B, C)\\)</p> </li> <li> <code>eps</code>             (<code>float</code>, default:                 <code>1e-05</code> )         \u2013          <p>a value added to the denominator for numerical stability. Default \\(1e-5\\).</p> </li> <li> <code>momentum</code>             (<code>float</code>, default:                 <code>0.1</code> )         \u2013          <p>the value used for the running mean and running var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: \\(0.1\\)</p> </li> <li> <code>affine</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></p> </li> <li> <code>track_running_stats</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to<code>False</code>, this module does not track such statistics, and initializes statistics buffers running_mean and running_var as None. When these buffers are None, this module always uses batch statistics. in both training and eval modes. Default: <code>True</code></p> </li> <li> <code>cdtype</code>             (<code>dtype</code>, default:                 <code>complex64</code> )         \u2013          <p>the dtype for complex numbers. Default torch.complex64</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/batchnorm.py</code> <pre><code>def __init__(\n    self,\n    num_features: int,\n    eps: float = 1e-5,\n    momentum: float = 0.1,\n    affine: bool = True,\n    track_running_stats: bool = True,\n    device: torch.device = None,\n    cdtype: torch.dtype = torch.complex64,\n) -&gt; None:\n    super().__init__()\n\n    self.num_features = num_features\n    self.eps = eps\n    self.momentum = momentum\n    self.affine = affine\n    self.track_running_stats = track_running_stats\n\n    if self.affine:\n        self.weight = torch.nn.parameter.Parameter(\n            torch.empty((num_features, 2, 2), device=device)\n        )\n        self.bias = torch.nn.parameter.Parameter(\n            torch.empty((num_features,), device=device, dtype=cdtype)\n        )\n    else:\n        self.register_parameter(\"weight\", None)\n        self.register_parameter(\"bias\", None)\n\n    if self.track_running_stats:\n        # Register the running mean and running variance\n        # These will not be returned by model.parameters(), hence\n        # not updated by the optimizer although returned in the state_dict\n        # and therefore stored as model's assets\n        self.register_buffer(\n            \"running_mean\",\n            torch.zeros((num_features,), device=device, dtype=cdtype),\n        )\n        self.register_buffer(\n            \"running_var\", torch.ones((num_features, 2, 2), device=device)\n        )\n        self.register_buffer(\n            \"num_batches_tracked\", torch.tensor(0, dtype=torch.long, device=device)\n        )\n    else:\n        self.register_buffer(\"running_mean\", None)\n        self.register_buffer(\"running_var\", None)\n        self.register_buffer(\n            \"num_batches_tracked\",\n            None,\n        )\n    self.reset_parameters()\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/batchnorm/#torchcvnn.nn.modules.batchnorm.BatchNorm2d","title":"BatchNorm2d","text":"<pre><code>BatchNorm2d(\n    num_features: int,\n    eps: float = 1e-05,\n    momentum: float = 0.1,\n    affine: bool = True,\n    track_running_stats: bool = True,\n    device: device = None,\n    cdtype: dtype = torch.complex64,\n)\n</code></pre> <p>             Bases: <code>_BatchNormNd</code></p> <p>BatchNorm for complex valued neural networks. The same code applies for BatchNorm1d, BatchNorm2d, the only condition being the input tensor must be (batch_size, features, d1, d2, ..)</p> <p>The statistics will be computed over the \\(batch\\_size \\times d_1 \\times d_2 \\times ..\\) vectors of size \\(features\\).</p> <p>As defined by Trabelsi et al. (2018)</p> <p>Parameters:</p> <ul> <li> <code>num_features</code>             (<code>int</code>)         \u2013          <p>\\(C\\) from an expected input of size \\((B, C)\\)</p> </li> <li> <code>eps</code>             (<code>float</code>, default:                 <code>1e-05</code> )         \u2013          <p>a value added to the denominator for numerical stability. Default \\(1e-5\\).</p> </li> <li> <code>momentum</code>             (<code>float</code>, default:                 <code>0.1</code> )         \u2013          <p>the value used for the running mean and running var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: \\(0.1\\)</p> </li> <li> <code>affine</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></p> </li> <li> <code>track_running_stats</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to<code>False</code>, this module does not track such statistics, and initializes statistics buffers running_mean and running_var as None. When these buffers are None, this module always uses batch statistics. in both training and eval modes. Default: <code>True</code></p> </li> <li> <code>cdtype</code>             (<code>dtype</code>, default:                 <code>complex64</code> )         \u2013          <p>the dtype for complex numbers. Default torch.complex64</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/batchnorm.py</code> <pre><code>def __init__(\n    self,\n    num_features: int,\n    eps: float = 1e-5,\n    momentum: float = 0.1,\n    affine: bool = True,\n    track_running_stats: bool = True,\n    device: torch.device = None,\n    cdtype: torch.dtype = torch.complex64,\n) -&gt; None:\n    super().__init__()\n\n    self.num_features = num_features\n    self.eps = eps\n    self.momentum = momentum\n    self.affine = affine\n    self.track_running_stats = track_running_stats\n\n    if self.affine:\n        self.weight = torch.nn.parameter.Parameter(\n            torch.empty((num_features, 2, 2), device=device)\n        )\n        self.bias = torch.nn.parameter.Parameter(\n            torch.empty((num_features,), device=device, dtype=cdtype)\n        )\n    else:\n        self.register_parameter(\"weight\", None)\n        self.register_parameter(\"bias\", None)\n\n    if self.track_running_stats:\n        # Register the running mean and running variance\n        # These will not be returned by model.parameters(), hence\n        # not updated by the optimizer although returned in the state_dict\n        # and therefore stored as model's assets\n        self.register_buffer(\n            \"running_mean\",\n            torch.zeros((num_features,), device=device, dtype=cdtype),\n        )\n        self.register_buffer(\n            \"running_var\", torch.ones((num_features, 2, 2), device=device)\n        )\n        self.register_buffer(\n            \"num_batches_tracked\", torch.tensor(0, dtype=torch.long, device=device)\n        )\n    else:\n        self.register_buffer(\"running_mean\", None)\n        self.register_buffer(\"running_var\", None)\n        self.register_buffer(\n            \"num_batches_tracked\",\n            None,\n        )\n    self.reset_parameters()\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/batchnorm/#torchcvnn.nn.modules.batchnorm.batch_cov","title":"batch_cov","text":"<pre><code>batch_cov(points: Tensor, centered: bool = False) -&gt; Tensor\n</code></pre> <p>Batched covariance computation Adapted from : https://stackoverflow.com/a/71357620/2164582</p> <p>Parameters:</p> <ul> <li> <code>points</code>             (<code>Tensor</code>)         \u2013          <p>the (B, N, D) input tensor from which to compute B covariances</p> </li> <li> <code>centered</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If <code>True</code>, assumes for every batch, the N vectors are centered.  default: <code>False</code></p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bcov</code> (            <code>Tensor</code> )        \u2013          <p>the covariances as a <code>(B, D, D)</code> tensor</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/batchnorm.py</code> <pre><code>def batch_cov(points: torch.Tensor, centered: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Batched covariance computation\n    Adapted from : https://stackoverflow.com/a/71357620/2164582\n\n    Arguments:\n        points: the (B, N, D) input tensor from which to compute B covariances\n        centered: If `True`, assumes for every batch, the N vectors are centered.  default: `False`\n\n    Returns:\n        bcov: the covariances as a `(B, D, D)` tensor\n    \"\"\"\n    B, N, D = points.size()\n    if not centered:\n        mean = points.mean(dim=1).unsqueeze(1)\n        diffs = (points - mean).reshape(B * N, D)\n    else:\n        diffs = points.reshape(B * N, D)\n    prods = torch.bmm(diffs.unsqueeze(2), diffs.unsqueeze(1)).reshape(B, N, D, D)\n    bcov = prods.sum(dim=1) / (N - 1)  # Unbiased estimate\n    return bcov  # (B, D, D)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/batchnorm/#torchcvnn.nn.modules.batchnorm.inv_2x2","title":"inv_2x2","text":"<pre><code>inv_2x2(M: Tensor) -&gt; Tensor\n</code></pre> <p>Computes the inverse of a tensor of shape [N, 2, 2].</p> <p>If we denote</p> \\[ M = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix} \\] <p>The inverse is given by</p> \\[ M^{-1} = \\frac{1}{Det M} Adj(M) = \\frac{1}{ad - bc}\\begin{pmatrix}d &amp; -b \\\\ -c &amp; a\\end{pmatrix} \\] <p>Parameters:</p> <ul> <li> <code>M</code>             (<code>Tensor</code>)         \u2013          <p>a batch of 2x2 tensors to invert, i.e. a \\((B, 2, 2)\\) tensor</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/batchnorm.py</code> <pre><code>def inv_2x2(M: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Computes the inverse of a tensor of shape [N, 2, 2].\n\n    If we denote\n\n    $$\n    M = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\n    $$\n\n    The inverse is given by\n\n    $$\n    M^{-1} = \\frac{1}{Det M} Adj(M) = \\frac{1}{ad - bc}\\begin{pmatrix}d &amp; -b \\\\ -c &amp; a\\end{pmatrix}\n    $$\n\n    Arguments:\n        M: a batch of 2x2 tensors to invert, i.e. a $(B, 2, 2)$ tensor\n    \"\"\"\n    det = torch.linalg.det(M).unsqueeze(-1).unsqueeze(-1)\n\n    M_adj = M.clone()\n    M_adj[:, 0, 0], M_adj[:, 1, 1] = M[:, 1, 1], M[:, 0, 0]\n    M_adj[:, 0, 1] *= -1\n    M_adj[:, 1, 0] *= -1\n    M_inv = 1 / det * M_adj\n    return M_inv\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/batchnorm/#torchcvnn.nn.modules.batchnorm.inv_sqrt_2x2","title":"inv_sqrt_2x2","text":"<pre><code>inv_sqrt_2x2(M: Tensor) -&gt; Tensor\n</code></pre> <p>Computes the square root of the inverse of a tensor of shape [N, 2, 2]</p> <p>Parameters:</p> <ul> <li> <code>M</code>             (<code>Tensor</code>)         \u2013          <p>a batch of 2x2 tensors to sqrt invert, i.e. a \\((B, 2, 2)\\) tensor</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/batchnorm.py</code> <pre><code>def inv_sqrt_2x2(M: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the square root of the inverse of a tensor of shape [N, 2, 2]\n\n    Arguments:\n        M: a batch of 2x2 tensors to sqrt invert, i.e. a $(B, 2, 2)$ tensor\n    \"\"\"\n    N = M.shape[0]\n    det = torch.linalg.det(M).unsqueeze(-1).unsqueeze(-1)\n    sqrt_det = torch.sqrt(det)\n\n    trace = torch.diagonal(M, dim1=-2, dim2=-1).sum(-1).unsqueeze(-1).unsqueeze(-1)\n    t = torch.sqrt(trace + 2 * sqrt_det)\n\n    M_adj = M.clone()\n    M_adj[:, 0, 0], M_adj[:, 1, 1] = M[:, 1, 1], M[:, 0, 0]\n    M_adj[:, 0, 1] *= -1\n    M_adj[:, 1, 0] *= -1\n    M_sqrt_inv = (\n        1 / t * (M_adj / sqrt_det + torch.eye(2, device=M.device).tile(N, 1, 1))\n    )\n    return M_sqrt_inv\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/batchnorm/#torchcvnn.nn.modules.batchnorm.slow_inv_sqrt_2x2","title":"slow_inv_sqrt_2x2","text":"<pre><code>slow_inv_sqrt_2x2(M: Tensor) -&gt; Tensor\n</code></pre> <p>Computes the square root of the inverse of a tensor of shape [N, 2, 2]</p> <p>Parameters:</p> <ul> <li> <code>M</code>             (<code>Tensor</code>)         \u2013          <p>a batch of 2x2 tensors to sqrt invert, i.e. a \\((B, 2, 2)\\) tensor</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/batchnorm.py</code> <pre><code>def slow_inv_sqrt_2x2(M: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the square root of the inverse of a tensor of shape [N, 2, 2]\n\n    Arguments:\n        M: a batch of 2x2 tensors to sqrt invert, i.e. a $(B, 2, 2)$ tensor\n    \"\"\"\n    return sqrt_2x2(inv_2x2(M))\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/batchnorm/#torchcvnn.nn.modules.batchnorm.sqrt_2x2","title":"sqrt_2x2","text":"<pre><code>sqrt_2x2(M: Tensor) -&gt; Tensor\n</code></pre> <p>Computes the square root of the tensor of shape [N, 2, 2].</p> <p>If we denote</p> \\[ M = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix} \\] <p>The square root is given by :</p> \\[ \\begin{align} \\sqrt{M} &amp;= \\frac{1}{t} ( M + \\sqrt{Det M} I)\\\\ t &amp;= \\sqrt{Tr M + 2 \\sqrt{Det M}} \\end{align} \\] <p>Parameters:</p> <ul> <li> <code>M</code>             (<code>Tensor</code>)         \u2013          <p>a batch of 2x2 tensors to invert, i.e. a \\((B, 2, 2)\\) tensor</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/batchnorm.py</code> <pre><code>def sqrt_2x2(M: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Computes the square root of the tensor of shape [N, 2, 2].\n\n    If we denote\n\n    $$\n    M = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\n    $$\n\n    The square root is given by :\n\n    $$\n    \\begin{align}\n    \\sqrt{M} &amp;= \\frac{1}{t} ( M + \\sqrt{Det M} I)\\\\\n    t &amp;= \\sqrt{Tr M + 2 \\sqrt{Det M}}\n    \\end{align}\n    $$\n\n    Arguments:\n        M: a batch of 2x2 tensors to invert, i.e. a $(B, 2, 2)$ tensor\n    \"\"\"\n    N = M.shape[0]\n    det = torch.linalg.det(M).unsqueeze(-1).unsqueeze(-1)\n    sqrt_det = torch.sqrt(det)\n\n    trace = torch.diagonal(M, dim1=-2, dim2=-1).sum(-1).unsqueeze(-1).unsqueeze(-1)\n    t = torch.sqrt(trace + 2 * sqrt_det)\n\n    sqrt_M = 1 / t * (M + sqrt_det * torch.eye(2, device=M.device).tile(N, 1, 1))\n    return sqrt_M\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/conv/","title":"conv","text":""},{"location":"reference/torchcvnn/nn/modules/conv/#torchcvnn.nn.modules.conv.ConvTranspose2d","title":"ConvTranspose2d","text":"<pre><code>ConvTranspose2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: _size_2_t = 0,\n    output_padding: _size_2_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_2_t = 1,\n    padding_mode: str = \"zeros\",\n    device=None,\n    dtype=None,\n)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Implementation of torch.nn.Conv2dTranspose for complex numbers. Apply Conv2dTranspose on real and imaginary part of the complex number.</p> <p>The parameters are the same than from the upstream pytorch layer:</p> <p>Parameters:</p> <ul> <li> <code>in_channels</code>             (<code>int</code>)         \u2013          <p>Number of channels in the input image</p> </li> <li> <code>out_channels</code>             (<code>int</code>)         \u2013          <p>Number of channels produced by the convolution</p> </li> <li> <code>kernel_size</code>             (<code>int or tuple</code>)         \u2013          <p>Size of the convolving kernel</p> </li> <li> <code>stride</code>             (<code>int or tuple</code>, default:                 <code>1</code> )         \u2013          <p>Stride of the convolution. Default: 1</p> </li> <li> <code>padding</code>             (<code>int or tuple</code>, default:                 <code>0</code> )         \u2013          <p><code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both sides of each dimension in the input. Default: 0</p> </li> <li> <code>output_padding</code>             (<code>int or tuple</code>, default:                 <code>0</code> )         \u2013          <p>Additional size added to one side of each dimension in the output shape. Default: 0</p> </li> <li> <code>groups</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Number of blocked connections from input channels to output channels. Default: 1</p> </li> <li> <code>bias</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p> </li> <li> <code>dilation</code>             (<code>int or tuple</code>, default:                 <code>1</code> )         \u2013          <p>Spacing between kernel elements. Default: 1</p> </li> <li> <code>padding_mode</code>             (<code>str</code>, default:                 <code>'zeros'</code> )         \u2013          <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code></p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: _size_2_t = 0,\n    output_padding: _size_2_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_2_t = 1,\n    padding_mode: str = \"zeros\",\n    device=None,\n    dtype=None,\n) -&gt; None:\n    super().__init__()\n\n    self.m_real = torch.nn.ConvTranspose2d(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        bias,\n        dilation,\n        padding_mode,\n        device,\n        dtype,\n    )\n\n    self.m_imag = torch.nn.ConvTranspose2d(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        bias,\n        dilation,\n        padding_mode,\n        device,\n        dtype,\n    )\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/conv/#torchcvnn.nn.modules.conv.ConvTranspose2d.forward","title":"forward","text":"<pre><code>forward(z: Tensor) -&gt; Tensor\n</code></pre> <p>Performs the forward pass, applying real valued ConvTranspose2d independently on both the real and imaginary parts of the input.</p> Source code in <code>src/torchcvnn/nn/modules/conv.py</code> <pre><code>def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs the forward pass, applying real valued ConvTranspose2d\n    independently on both the real and imaginary parts of the input.\n    \"\"\"\n    return torch.view_as_complex(\n        torch.cat(\n            (\n                torch.unsqueeze(self.m_real(z.real) - self.m_imag(z.imag), -1),\n                torch.unsqueeze(self.m_real(z.imag) + self.m_imag(z.real), -1),\n            ),\n            axis=-1,\n        )\n    )\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/dropout/","title":"dropout","text":""},{"location":"reference/torchcvnn/nn/modules/dropout/#torchcvnn.nn.modules.dropout.Dropout","title":"Dropout","text":"<pre><code>Dropout(p: float = 0.5)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>As from the upstream pytorch :</p> <p>During training, randomly zeroes some of the elements of the input tensor with probability <code>p</code> using samples from a Bernouilli distribution. Each channel will be zeroed out independently on every forward call.</p> <p>Furthermore, the outputs are scaled by a factor of \\(\\frac{1}{1-p}\\) during training. This means that during evaluation the module simply computes an identity function.</p> Note <p>As for now, with torch 2.1.0 torch.nn.Dropout cannot be applied on complex valued tensors. However, our implementation relies on the torch implementation by dropout out a tensor of ones used as a mask on the input.</p> <p>Parameters:</p> <ul> <li> <code>p</code>             (<code>float</code>, default:                 <code>0.5</code> )         \u2013          <p>probability of an element to be zeroed.</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/dropout.py</code> <pre><code>def __init__(self, p: float = 0.5):\n    super().__init__()\n    self.p = p\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/dropout/#torchcvnn.nn.modules.dropout.Dropout2d","title":"Dropout2d","text":"<pre><code>Dropout2d(p: float = 0.5)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>As from the upstream pytorch : applies dropout to zero out complete channels</p> Note <p>As for now, with torch 2.1.0 torch.nn.Dropout2d cannot be applied on complex valued tensors. However, our implementation relies on the torch implementation by dropout out a tensor of ones used as a mask on the input.</p> <p>Parameters:</p> <ul> <li> <code>p</code>             (<code>float</code>, default:                 <code>0.5</code> )         \u2013          <p>probability of an element to be zeroed.</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/dropout.py</code> <pre><code>def __init__(self, p: float = 0.5):\n    super().__init__()\n    self.p = p\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/initialization/","title":"initialization","text":""},{"location":"reference/torchcvnn/nn/modules/initialization/#torchcvnn.nn.modules.initialization.complex_kaiming_normal_","title":"complex_kaiming_normal_","text":"<pre><code>complex_kaiming_normal_(\n    tensor: Tensor,\n    a: float = 0,\n    mode: str = \"fan_in\",\n    nonlinearity: str = \"leaky_relu\",\n) -&gt; Tensor\n</code></pre> <p>Fills the input <code>Tensor</code> with values according to the method described in <code>Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</code> - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from \\(\\mathcal{N}(0, \\text{std}^2)\\) where</p> \\[ \\text{std} = \\frac{\\text{gain}}{\\sqrt{2\\text{fan_mode}}} \\] <p>Also known as He initialization.</p> <p>Parameters:</p> <ul> <li> <code>tensor</code>             (<code>Tensor</code>)         \u2013          <p>an n-dimensional <code>torch.Tensor</code></p> </li> <li> <code>a</code>             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>the negative slope of the rectifier used after this layer (only used with <code>'leaky_relu'</code>)</p> </li> <li> <code>mode</code>             (<code>str</code>, default:                 <code>'fan_in'</code> )         \u2013          <p>either <code>'fan_in'</code> (default) or <code>'fan_out'</code>. Choosing <code>'fan_in'</code> preserves the magnitude of the variance of the weights in the forward pass. Choosing <code>'fan_out'</code> preserves the magnitudes in the backwards pass.</p> </li> <li> <code>nonlinearity</code>             (<code>str</code>, default:                 <code>'leaky_relu'</code> )         \u2013          <p>the non-linear function (<code>nn.functional</code> name), recommended to use only with <code>'relu'</code> or <code>'leaky_relu'</code> (default).</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5, dtype=torch.complex64)\n&gt;&gt;&gt; c_nn.init.complex_kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n</code></pre> <p>This implementation is a minor adaptation of the torch.nn.init.kaiming_normal_ function</p> Source code in <code>src/torchcvnn/nn/modules/initialization.py</code> <pre><code>def complex_kaiming_normal_(\n    tensor: torch.Tensor,\n    a: float = 0,\n    mode: str = \"fan_in\",\n    nonlinearity: str = \"leaky_relu\",\n) -&gt; torch.Tensor:\n    r\"\"\"Fills the input `Tensor` with values according to the method\n    described in `Delving deep into rectifiers: Surpassing human-level\n    performance on ImageNet classification` - He, K. et al. (2015), using a\n    normal distribution. The resulting tensor will have values sampled from\n    $\\mathcal{N}(0, \\text{std}^2)$ where\n\n    $$\n    \\text{std} = \\frac{\\text{gain}}{\\sqrt{2\\text{fan_mode}}}\n    $$\n\n    Also known as He initialization.\n\n    Arguments:\n        tensor: an n-dimensional `torch.Tensor`\n        a: the negative slope of the rectifier used after this layer (only\n            used with ``'leaky_relu'``)\n        mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\n            preserves the magnitude of the variance of the weights in the\n            forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the\n            backwards pass.\n        nonlinearity: the non-linear function (`nn.functional` name),\n            recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).\n\n    Examples:\n        &gt;&gt;&gt; w = torch.empty(3, 5, dtype=torch.complex64)\n        &gt;&gt;&gt; c_nn.init.complex_kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n\n    This implementation is a minor adaptation of the torch.nn.init.kaiming_normal_ function\n    \"\"\"\n\n    fan = nn.init._calculate_correct_fan(tensor, mode)\n    gain = nn.init.calculate_gain(nonlinearity, a)\n    std = (gain / math.sqrt(fan)) / math.sqrt(2)\n\n    return nn.init._no_grad_normal_(tensor, 0.0, std)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/initialization/#torchcvnn.nn.modules.initialization.complex_kaiming_uniform_","title":"complex_kaiming_uniform_","text":"<pre><code>complex_kaiming_uniform_(\n    tensor: Tensor,\n    a: float = 0,\n    mode: str = \"fan_in\",\n    nonlinearity: str = \"leaky_relu\",\n) -&gt; Tensor\n</code></pre> <p>Fills the input <code>Tensor</code> with values according to the method described in <code>Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</code> - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from \\(\\mathcal{U}(-\\text{bound}, \\text{bound})\\) where</p> \\[ \\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{2\\text{fan_mode}}} \\] <p>Also known as He initialization.</p> <p>Parameters:</p> <ul> <li> <code>tensor</code>             (<code>Tensor</code>)         \u2013          <p>an n-dimensional <code>torch.Tensor</code></p> </li> <li> <code>a</code>             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>the negative slope of the rectifier used after this layer (only used with <code>'leaky_relu'</code>)</p> </li> <li> <code>mode</code>             (<code>str</code>, default:                 <code>'fan_in'</code> )         \u2013          <p>either <code>'fan_in'</code> (default) or <code>'fan_out'</code>. Choosing <code>'fan_in'</code> preserves the magnitude of the variance of the weights in the forward pass. Choosing <code>'fan_out'</code> preserves the magnitudes in the backwards pass.</p> </li> <li> <code>nonlinearity</code>             (<code>str</code>, default:                 <code>'leaky_relu'</code> )         \u2013          <p>the non-linear function (<code>nn.functional</code> name), recommended to use only with <code>'relu'</code> or <code>'leaky_relu'</code> (default).</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5, dtype=torch.complex64)\n&gt;&gt;&gt; c_nn.init.complex_kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n</code></pre> <p>This implementation is a minor adaptation of the torch.nn.init.kaiming_uniform_ function</p> Source code in <code>src/torchcvnn/nn/modules/initialization.py</code> <pre><code>def complex_kaiming_uniform_(\n    tensor: torch.Tensor,\n    a: float = 0,\n    mode: str = \"fan_in\",\n    nonlinearity: str = \"leaky_relu\",\n) -&gt; torch.Tensor:\n    r\"\"\"Fills the input `Tensor` with values according to the method\n    described in `Delving deep into rectifiers: Surpassing human-level\n    performance on ImageNet classification` - He, K. et al. (2015), using a\n    uniform distribution. The resulting tensor will have values sampled from\n    $\\mathcal{U}(-\\text{bound}, \\text{bound})$ where\n\n    $$\n    \\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{2\\text{fan_mode}}}\n    $$\n\n    Also known as He initialization.\n\n    Arguments:\n        tensor: an n-dimensional `torch.Tensor`\n        a: the negative slope of the rectifier used after this layer (only\n            used with ``'leaky_relu'``)\n        mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\n            preserves the magnitude of the variance of the weights in the\n            forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the\n            backwards pass.\n        nonlinearity: the non-linear function (`nn.functional` name),\n            recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).\n\n    Examples:\n        &gt;&gt;&gt; w = torch.empty(3, 5, dtype=torch.complex64)\n        &gt;&gt;&gt; c_nn.init.complex_kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n\n    This implementation is a minor adaptation of the torch.nn.init.kaiming_uniform_ function\n    \"\"\"\n\n    if 0 in tensor.shape:\n        warnings.warn(\"Initializing zero-element tensors is a no-op\")\n        return tensor\n    fan = nn.init._calculate_correct_fan(tensor, mode)\n    gain = nn.init.calculate_gain(nonlinearity, a)\n    std = gain / math.sqrt(fan) / math.sqrt(2)\n    bound = math.sqrt(3.0) * std  # Calculate uniform bounds from standard deviation\n\n    return nn.init._no_grad_uniform_(tensor, -bound, bound)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/initialization/#torchcvnn.nn.modules.initialization.complex_xavier_normal_","title":"complex_xavier_normal_","text":"<pre><code>complex_xavier_normal_(\n    tensor: Tensor,\n    a: float = 0,\n    nonlinearity: str = \"leaky_relu\",\n) -&gt; Tensor\n</code></pre> <p>Fills the input <code>Tensor</code> with values according to the method described in <code>Understanding the difficulty of training deep feedforward neural networks</code> - Glorot, X. &amp; Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from \\(\\mathcal{N}(0, \\text{std}^2)\\) where</p> \\[ \\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{2(\\text{fan_in} + \\text{fan_out})}} \\] <p>Also known as Glorot initialization.</p> <p>Parameters:</p> <ul> <li> <code>tensor</code>             (<code>Tensor</code>)         \u2013          <p>an n-dimensional <code>torch.Tensor</code></p> </li> <li> <code>a</code>             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>an optional parameter to the non-linear function</p> </li> <li> <code>nonlinearity</code>             (<code>str</code>, default:                 <code>'leaky_relu'</code> )         \u2013          <p>the non linearity to compute the gain</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5, dtype=torch.complex64)\n&gt;&gt;&gt; nn.init.complex_xavier_normal_(w, nonlinearity='relu')\n</code></pre> <p>This implementation is a minor adaptation of the torch.nn.init.xavier_normal_ function</p> Source code in <code>src/torchcvnn/nn/modules/initialization.py</code> <pre><code>def complex_xavier_normal_(\n    tensor: torch.Tensor,\n    a: float = 0,\n    nonlinearity: str = \"leaky_relu\",\n) -&gt; torch.Tensor:\n    r\"\"\"Fills the input `Tensor` with values according to the method\n    described in `Understanding the difficulty of training deep feedforward\n    neural networks` - Glorot, X. &amp; Bengio, Y. (2010), using a normal\n    distribution. The resulting tensor will have values sampled from\n    $\\mathcal{N}(0, \\text{std}^2)$ where\n\n    $$\n    \\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{2(\\text{fan_in} + \\text{fan_out})}}\n    $$\n\n    Also known as Glorot initialization.\n\n    Arguments:\n        tensor: an n-dimensional `torch.Tensor`\n        a: an optional parameter to the non-linear function\n        nonlinearity: the non linearity to compute the gain\n\n    Examples:\n        &gt;&gt;&gt; w = torch.empty(3, 5, dtype=torch.complex64)\n        &gt;&gt;&gt; nn.init.complex_xavier_normal_(w, nonlinearity='relu')\n\n    This implementation is a minor adaptation of the torch.nn.init.xavier_normal_ function\n    \"\"\"\n    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(tensor)\n    gain = nn.init.calculate_gain(nonlinearity, a)\n    std = (gain * math.sqrt(2.0 / float(fan_in + fan_out))) / math.sqrt(2)\n\n    return nn.init._no_grad_normal_(tensor, 0.0, std)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/initialization/#torchcvnn.nn.modules.initialization.complex_xavier_uniform_","title":"complex_xavier_uniform_","text":"<pre><code>complex_xavier_uniform_(\n    tensor: Tensor,\n    a: float = 0,\n    nonlinearity: str = \"leaky_relu\",\n) -&gt; Tensor\n</code></pre> <p>Fills the input <code>Tensor</code> with values according to the method described in <code>Understanding the difficulty of training deep feedforward neural networks</code> - Glorot, X. &amp; Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from \\(\\mathcal{U}(-a, a)\\) where</p> \\[ a = \\text{gain} \\times \\sqrt{\\frac{6}{2(\\text{fan_in} + \\text{fan_out})}} \\] <p>Also known as Glorot initialization.</p> <p>Parameters:</p> <ul> <li> <code>tensor</code>             (<code>Tensor</code>)         \u2013          <p>an n-dimensional <code>torch.Tensor</code></p> </li> <li> <code>a</code>             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>an optional parameter to the non-linear function</p> </li> <li> <code>nonlinearity</code>             (<code>str</code>, default:                 <code>'leaky_relu'</code> )         \u2013          <p>the non linearity to compute the gain</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5, dtype=torch.complex64)\n&gt;&gt;&gt; c_nn.init.complex_xavier_uniform_(w, nonlinearity='relu')\n</code></pre> <p>This implementation is a minor adaptation of the torch.nn.init.xavier_uniform_ function</p> Source code in <code>src/torchcvnn/nn/modules/initialization.py</code> <pre><code>def complex_xavier_uniform_(\n    tensor: torch.Tensor,\n    a: float = 0,\n    nonlinearity: str = \"leaky_relu\",\n) -&gt; torch.Tensor:\n    r\"\"\"Fills the input `Tensor` with values according to the method\n    described in `Understanding the difficulty of training deep feedforward\n    neural networks` - Glorot, X. &amp; Bengio, Y. (2010), using a uniform\n    distribution. The resulting tensor will have values sampled from\n    $\\mathcal{U}(-a, a)$ where\n\n    $$\n    a = \\text{gain} \\times \\sqrt{\\frac{6}{2(\\text{fan_in} + \\text{fan_out})}}\n    $$\n\n    Also known as Glorot initialization.\n\n    Arguments:\n        tensor: an n-dimensional `torch.Tensor`\n        a: an optional parameter to the non-linear function\n        nonlinearity: the non linearity to compute the gain\n\n    Examples:\n        &gt;&gt;&gt; w = torch.empty(3, 5, dtype=torch.complex64)\n        &gt;&gt;&gt; c_nn.init.complex_xavier_uniform_(w, nonlinearity='relu')\n\n    This implementation is a minor adaptation of the torch.nn.init.xavier_uniform_ function\n    \"\"\"\n    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(tensor)\n    gain = nn.init.calculate_gain(nonlinearity, a)\n    std = gain * math.sqrt(2.0 / float(fan_in + fan_out)) / math.sqrt(2)\n    bound = math.sqrt(3.0) * std  # Calculate uniform bounds from standard deviation\n\n    return nn.init._no_grad_uniform_(tensor, -bound, bound)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/loss/","title":"loss","text":""},{"location":"reference/torchcvnn/nn/modules/loss/#torchcvnn.nn.modules.loss.ComplexMSELoss","title":"ComplexMSELoss","text":"<pre><code>ComplexMSELoss(\n    size_average=None, reduce=None, reduction: str = \"mean\"\n)\n</code></pre> <p>             Bases: <code>_Loss</code></p> <p>A custom implementation of Mean Squared Error (MSE) Loss for complex-valued inputs.</p> <p>This loss function is designed to compute the mean squared error between two sets of complex-valued tensors. Unlike the standard MSE loss which directly computes the square of the difference between predicted values and true values for real numbers, this implementation adapts to complex numbers by calculating the squared magnitude of the difference between the complex predicted and true values before averaging.</p> <p>For complex-valued numbers, the MSE is defined as:     MSE = mean(|y_true - y_pred|^2) where |y_true - y_pred| denotes the magnitude of the complex difference between the true and predicted values. This formula ensures that both the real and imaginary parts of the complex numbers are considered in the loss calculation.</p> <p>For real-valued numbers, the standard MSE formula is:     MSE = mean((y_true - y_pred)^2) where (y_true - y_pred)^2 is the square of the difference between the true and predicted values.</p> <p>Parameters:</p> <ul> <li> <code>size_average</code>             (<code>bool</code>, default:                 <code>None</code> )         \u2013          <p>Deprecated (use reduction). By default, the losses are averaged over each loss element in the batch. Note: when (reduce is False), size_average is ignored. Default is None.</p> </li> <li> <code>reduce</code>             (<code>bool</code>, default:                 <code>None</code> )         \u2013          <p>Deprecated (use reduction). By default, the losses are averaged or summed over observations for each minibatch depending on reduction. Note: when (reduce is False), size_average is ignored. Default is None.</p> </li> <li> <code>reduction</code>             (<code>str</code>, default:                 <code>'mean'</code> )         \u2013          <p>Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Default is 'mean'.</p> </li> </ul> Inputs <p>y_pred (torch.Tensor): The predicted values, can be a complex tensor. y_true (torch.Tensor): The ground truth values, must have the same shape as the predicted values, can be a complex tensor.</p> <p>Returns:</p> <ul> <li>         \u2013          <p>torch.Tensor: The calculated mean squared error loss.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; loss = ComplexMSELoss()\n&gt;&gt;&gt; y_pred = torch.tensor([1+1j, 2+2j], dtype=torch.complex64)\n&gt;&gt;&gt; y_true = torch.tensor([1+2j, 2+3j], dtype=torch.complex64)\n&gt;&gt;&gt; output = loss(y_pred, y_true)\n&gt;&gt;&gt; print(output)\n</code></pre> Source code in <code>src/torchcvnn/nn/modules/loss.py</code> <pre><code>def __init__(self, size_average=None, reduce=None, reduction: str = \"mean\") -&gt; None:\n    super().__init__(size_average, reduce, reduction)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/normalization/","title":"normalization","text":""},{"location":"reference/torchcvnn/nn/modules/normalization/#torchcvnn.nn.modules.normalization.LayerNorm","title":"LayerNorm","text":"<pre><code>LayerNorm(\n    normalized_shape: _shape_t,\n    elementwise_affine: bool = True,\n    bias: bool = True,\n    device: device = None,\n    dtype: dtype = torch.complex64,\n)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Implementation of the torch.nn.LayerNorm for complex numbers.</p> <p>Parameters:</p> <ul> <li> <code>normalized_shape</code>             (<code>int or list or Size</code>)         \u2013          <p>input shape from an expected input of size $(*, normalized_shape[0], normalized_shape[1], ..., normalized_shape[-1])</p> </li> <li> <code>elementwise_affine</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>a boolean value that when set to <code>True</code>, this module has learnable per-element affine parameters initialized to a diagonal matrix with diagonal element \\(1/\\sqrt{2}\\) (for weights) and zeros (for biases). Default: <code>True</code></p> </li> <li> <code>bias</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if set to <code>False</code>, the layer will not learn an additive bias</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/normalization.py</code> <pre><code>def __init__(\n    self,\n    normalized_shape: _shape_t,\n    elementwise_affine: bool = True,\n    bias: bool = True,\n    device: torch.device = None,\n    dtype: torch.dtype = torch.complex64,\n) -&gt; None:\n    super().__init__()\n    if isinstance(normalized_shape, numbers.Integral):\n        normalized_shape = (normalized_shape,)\n    self.normalized_shape = tuple(normalized_shape)\n\n    self.elementwise_affine = elementwise_affine\n\n    self.combined_dimensions = functools.reduce(operator.mul, self.normalized_shape)\n    if self.elementwise_affine:\n        self.weight = torch.nn.parameter.Parameter(\n            torch.empty((self.combined_dimensions, 2, 2), device=device)\n        )\n        if bias:\n            self.bias = torch.nn.parameter.Parameter(\n                torch.empty((self.combined_dimensions,), device=device, dtype=dtype)\n            )\n        else:\n            self.register_parameter(\"bias\", None)\n    else:\n        self.register_parameter(\"weight\", None)\n        self.register_parameter(\"bias\", None)\n    self.reset_parameters()\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/normalization/#torchcvnn.nn.modules.normalization.LayerNorm.forward","title":"forward","text":"<pre><code>forward(z: Tensor) -&gt; Tensor\n</code></pre> <p>Performs the forward pass</p> Source code in <code>src/torchcvnn/nn/modules/normalization.py</code> <pre><code>def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs the forward pass\n    \"\"\"\n    # z: *, normalized_shape[0] , ..., normalized_shape[-1]\n    z_ravel = z.view(-1, self.combined_dimensions).transpose(0, 1)\n\n    # Compute the means\n    mus = z_ravel.mean(axis=-1)  # normalized_shape[0]x normalized_shape[1], ...\n\n    # Center the inputs\n    z_centered = z_ravel - mus.unsqueeze(-1)\n    z_centered = torch.view_as_real(\n        z_centered\n    )  # combined_dimensions,num_samples, 2\n\n    # Transform the complex numbers as 2 reals to compute the variances and\n    # covariances\n    covs = bn.batch_cov(z_centered, centered=True)\n\n    # Invert the covariance to scale\n    invsqrt_covs = bn.inv_sqrt_2x2(covs)  # combined_dimensions, 2, 2\n    # Note: the z_centered.transpose is to make\n    # z_centered from (combined_dimensions, num_samples, 2) to (combined_dimensions, 2, num_samples)\n    # So that the batch matrix multiply works as expected\n    # where invsqrt_covs is (combined_dimensions, 2, 2)\n    outz = torch.bmm(invsqrt_covs, z_centered.transpose(1, 2))\n    outz = outz.contiguous()  # num_features, 2, BxHxW\n\n    # Shift by beta and scale by gamma\n    # weight is (num_features, 2, 2) real valued\n    outz = torch.bmm(self.weight, outz)  # combined_dimensions, 2, num_samples\n    outz = outz.transpose(1, 2).contiguous()  # combined_dimensions, num_samples, 2\n    outz = torch.view_as_complex(outz)  # combined_dimensions, num_samples\n\n    # bias is (C, ) complex dtype\n    outz += self.bias.view(-1, 1)\n\n    outz = outz.transpose(0, 1).contiguous()  # num_samples, comnbined_dimensions\n\n    outz = outz.view(z.shape)\n\n    return outz\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/normalization/#torchcvnn.nn.modules.normalization.LayerNorm.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Initialize the weight and bias. The weight is initialized to a diagonal matrix with diagonal \\(\\frac{1}{\\sqrt{2}}\\).</p> <p>The bias is initialized to \\(0\\).</p> Source code in <code>src/torchcvnn/nn/modules/normalization.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    r\"\"\"\n    Initialize the weight and bias. The weight is initialized to a diagonal\n    matrix with diagonal $\\frac{1}{\\sqrt{2}}$.\n\n    The bias is initialized to $0$.\n    \"\"\"\n    with torch.no_grad():\n        if self.elementwise_affine:\n            # Initialize all the weights to zeros\n            init.zeros_(self.weight)\n            # And then fill in the diagonal with 1/sqrt(2)\n            # w is C, 2, 2\n            self.weight.view(-1, 2, 2)[:, 0, 0] = 1 / math.sqrt(2.0)\n            self.weight.view(-1, 2, 2)[:, 1, 1] = 1 / math.sqrt(2.0)\n            # Initialize all the biases to zero\n            init.zeros_(self.bias)\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/pooling/","title":"pooling","text":""},{"location":"reference/torchcvnn/nn/modules/pooling/#torchcvnn.nn.modules.pooling.AvgPool2d","title":"AvgPool2d","text":"<pre><code>AvgPool2d(\n    kernel_size: _size_2_t,\n    stride: Optional[_size_2_t] = None,\n    padding: _size_2_t = 0,\n    ceil_mode: bool = False,\n    count_include_pad: bool = True,\n    divisor_override: Optional[int] = None,\n)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Implementation of torch.nn.AvgPool2d for complex numbers. Apply AvgPool2d on the real and imaginary part. Returns complex values associated to the AvgPool2d results.</p> <p>Parameters:</p> <ul> <li> <code>kernel_size</code>             (<code>_size_2_t</code>)         \u2013          <p>thr size of the window to compute the average</p> </li> <li> <code>stride</code>             (<code>Optional[_size_2_t]</code>, default:                 <code>None</code> )         \u2013          <p>the stride of the window</p> </li> <li> <code>padding</code>             (<code>_size_2_t</code>, default:                 <code>0</code> )         \u2013          <p>implicit negative infinity padding to be added</p> </li> <li> <code>ceil_mode</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>when <code>True</code>, use <code>ceil</code> instead of <code>floor</code> to compute the output shape</p> </li> <li> <code>count_include_pad</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>when <code>True</code>, will include the zero-padding in the averaging calculation</p> </li> <li> <code>divisor_override</code>             (<code>Optional[int]</code>, default:                 <code>None</code> )         \u2013          <p>if specified, it will be used as divisor, otherwise size of the pooling region will be used.</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/pooling.py</code> <pre><code>def __init__(\n    self,\n    kernel_size: _size_2_t,\n    stride: Optional[_size_2_t] = None,\n    padding: _size_2_t = 0,\n    ceil_mode: bool = False,\n    count_include_pad: bool = True,\n    divisor_override: Optional[int] = None,\n) -&gt; None:\n    super().__init__()\n    if type(kernel_size) == int:\n        self.kernel_size = [kernel_size] * 2 + [1]\n    elif type(kernel_size) == tuple:\n        if len(kernel_size) &lt; 3:\n            self.kernel_size = [kernel_size] + [1]\n        else:\n            self.kernel_size = kernel_size\n\n    if type(stride) == int:\n        self.stride = [stride] * 2 + [1]\n    elif type(stride) == tuple:\n        if len(stride) &lt; 3:\n            self.stride = [stride] + [1]\n        else:\n            self.stride = stride\n\n    if type(padding) == int:\n        self.padding = [padding] * 2 + [0]\n    elif type(padding) == tuple:\n        if len(padding) &lt; 3:\n            self.padding = [padding] + [0]\n        else:\n            self.padding = padding\n\n    self.m = torch.nn.AvgPool3d(\n        self.kernel_size,\n        self.stride,\n        self.padding,\n        ceil_mode,\n        count_include_pad,\n        divisor_override,\n    )\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/pooling/#torchcvnn.nn.modules.pooling.AvgPool2d.forward","title":"forward","text":"<pre><code>forward(z: Tensor) -&gt; Tensor\n</code></pre> <p>Computes the average over the real and imaginery parts.</p> Source code in <code>src/torchcvnn/nn/modules/pooling.py</code> <pre><code>def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the average over the real and imaginery parts.\n    \"\"\"\n    return torch.view_as_complex(self.m(torch.view_as_real(z)))\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/pooling/#torchcvnn.nn.modules.pooling.MaxPool2d","title":"MaxPool2d","text":"<pre><code>MaxPool2d(\n    kernel_size: _size_2_t,\n    stride: Optional[_size_2_t] = None,\n    padding: _size_2_t = 0,\n    dilation: _size_2_t = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False,\n)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Applies a 2D max pooling on the module of the input signal</p> <p>In the simplest case, the output value of the layer with input size \\((N, C, H, W)\\), output \\((N, C, H_{out}, W_{out})\\) and <code>kernel_size</code> <code>(kH, kW)</code> can be precisely described as:</p> \\[ \\begin{aligned} out(N_i, C_j, h, w) ={} &amp; \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\                         &amp; \\text{|input|}(N_i, C_j, \\text{stride[0]} \\times h + m,                                                \\text{stride[1]} \\times w + n) \\end{aligned} \\] <p>Internally, it is relying on the <code>torch.nn.MaxPool2d</code></p> <p>Parameters:</p> <ul> <li> <code>kernel_size</code>             (<code>_size_2_t</code>)         \u2013          <p>thr size of the window to take a max over</p> </li> <li> <code>stride</code>             (<code>Optional[_size_2_t]</code>, default:                 <code>None</code> )         \u2013          <p>the stride of the window</p> </li> <li> <code>padding</code>             (<code>_size_2_t</code>, default:                 <code>0</code> )         \u2013          <p>implicit negative infinity padding to be added</p> </li> <li> <code>dilation</code>             (<code>_size_2_t</code>, default:                 <code>1</code> )         \u2013          <p>a parameter that controls the stride of elements in the window</p> </li> <li> <code>ceil_mode</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>when <code>True</code>, use <code>ceil</code> instead of <code>floor</code> to compute the output shape</p> </li> <li> <code>return_indices</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if <code>True</code>, will return the max indices along with the outputs</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/pooling.py</code> <pre><code>def __init__(\n    self,\n    kernel_size: _size_2_t,\n    stride: Optional[_size_2_t] = None,\n    padding: _size_2_t = 0,\n    dilation: _size_2_t = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False,\n) -&gt; None:\n    super().__init__()\n    self.return_indices = return_indices\n    self.m = nn.MaxPool2d(\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        ceil_mode=ceil_mode,\n        return_indices=True,\n    )\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/pooling/#torchcvnn.nn.modules.pooling.MaxPool2d.forward","title":"forward","text":"<pre><code>forward(z: Tensor) -&gt; Tensor\n</code></pre> <p>Computes and return the MaxPool over the magnitude of the input</p> Source code in <code>src/torchcvnn/nn/modules/pooling.py</code> <pre><code>def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes and return the MaxPool over the magnitude of the input\n    \"\"\"\n    _, indices = self.m(torch.abs(z))\n\n    if self.return_indices:\n        return z.flatten()[indices], indices\n    else:\n        return z.flatten()[indices]\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/upsampling/","title":"upsampling","text":""},{"location":"reference/torchcvnn/nn/modules/upsampling/#torchcvnn.nn.modules.upsampling.Upsample","title":"Upsample","text":"<pre><code>Upsample(\n    size: Optional[_size_any_t] = None,\n    scale_factor: Optional[_ratio_any_t] = None,\n    mode: str = \"nearest\",\n    align_corners: Optional[bool] = None,\n    recompute_scale_factor: Optional[bool] = None,\n)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Works by applying independently the same upsampling to both the real and imaginary parts.</p> Note <p>With pytorch 2.1, applying the nn.Upsample to a complex valued tensor raises an exception \"compute_indices_weights_nearest\" not implemented for 'ComplexFloat' So it basically splits the input tensors in its real and imaginery parts, applies nn.Upsample on both components and view them as complex.</p> <p>Parameters:</p> <ul> <li> <code>size</code>             (<code>int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]</code>, default:                 <code>None</code> )         \u2013          <p>output spatial sizes</p> </li> <li> <code>scale_factor</code>             (<code>float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float]</code>, default:                 <code>None</code> )         \u2013          <p>multiplier for spatial size. Has to match input size if it is a tuple.</p> </li> <li> <code>mode</code>             (<code>str</code>, default:                 <code>'nearest'</code> )         \u2013          <p>the upsampling algorithm: one of <code>'nearest'</code>, <code>'linear'</code>, <code>'bilinear'</code>, <code>'bicubic'</code> and <code>'trilinear'</code>. Default: <code>'nearest'</code></p> </li> <li> <code>align_corners</code>             (<code>bool</code>, default:                 <code>None</code> )         \u2013          <p>if <code>True</code>, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when :attr:<code>mode</code> is <code>'linear'</code>, <code>'bilinear'</code>, <code>'bicubic'</code>, or <code>'trilinear'</code>. Default: <code>False</code></p> </li> <li> <code>recompute_scale_factor</code>             (<code>bool</code>, default:                 <code>None</code> )         \u2013          <p>recompute the scale_factor for use in the interpolation calculation. If <code>recompute_scale_factor</code> is <code>True</code>, then <code>scale_factor</code> must be passed in and <code>scale_factor</code> is used to compute the output <code>size</code>. The computed output <code>size</code> will be used to infer new scales for the interpolation. Note that when <code>scale_factor</code> is floating-point, it may differ from the recomputed <code>scale_factor</code> due to rounding and precision issues. If <code>recompute_scale_factor</code> is <code>False</code>, then <code>size</code> or <code>scale_factor</code> will be used directly for interpolation.</p> </li> </ul> Source code in <code>src/torchcvnn/nn/modules/upsampling.py</code> <pre><code>def __init__(\n    self,\n    size: Optional[_size_any_t] = None,\n    scale_factor: Optional[_ratio_any_t] = None,\n    mode: str = \"nearest\",\n    align_corners: Optional[bool] = None,\n    recompute_scale_factor: Optional[bool] = None,\n) -&gt; None:\n    super().__init__()\n    self.up_module = nn.Upsample(\n        size, scale_factor, mode, align_corners, recompute_scale_factor\n    )\n</code></pre>"},{"location":"reference/torchcvnn/nn/modules/upsampling/#torchcvnn.nn.modules.upsampling.Upsample.forward","title":"forward","text":"<pre><code>forward(z: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the forward pass</p> Source code in <code>src/torchcvnn/nn/modules/upsampling.py</code> <pre><code>def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the forward pass\n    \"\"\"\n    up_real = self.up_module(z.real).unsqueeze(-1)\n    up_imag = self.up_module(z.imag).unsqueeze(-1)\n    up_z = torch.cat((up_real, up_imag), axis=-1)\n    return torch.view_as_complex(up_z)\n</code></pre>"}]}